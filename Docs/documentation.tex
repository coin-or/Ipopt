%% Copyright (C) 2005, Carnegie Mellon University and others.
%%
%% The first version of this files was contributed to the Ipopt project
%% on Aug 1, 2005, by Yoshiaki Kawajiri
%%                    Department of Chemical Engineering
%%                    Carnegie Mellon University
%%                    Pittsburgh, PA 15213
%%
\documentclass[letter,10pt]{article}
\setlength{\textwidth}{6.3in}       % Text width
\setlength{\textheight}{8.5in}      % Text height
\setlength{\oddsidemargin}{0.1in}     % Left margin for even-numbered pages
\setlength{\evensidemargin}{0.1in}    % Left margin for odd-numbered pages
\setlength{\topmargin}{-0.5in}         % Top margin
\renewcommand{\baselinestretch}{1.1}
\usepackage{amsfonts}

\newcommand{\RR}{{\mathbb{R}}}


\begin{document}
\title{Introduction to Ipopt:\\
A tutorial for downloading, installing, and using Ipopt.}

\author{Yoshiaki
Kawajiri\footnote{Department of Chemical Engineering, Carnegie Mellon
University, Pittsburgh, PA, 15213, Email: kawajiri@cmu.edu}, Carl
D. Laird\footnote{Department of Chemical Engineering, Carnegie Mellon
University, Pittsburgh, PA, 15213, Email: kawajiri@cmu.edu}}

%\date{\today}
\maketitle

\tableofcontents

\section{Overview}
Ipopt (\underline{I}nterior \underline{P}oint \underline{Opt}imizer) is an open
source software package for large-scale nonlinear optimization. It can
be used to solve general nonlinear programming problems of the form
\begin{eqnarray}
\min_{x} &&f(x) \label{obj} \\
\mbox{s.t.} \;  &&g^L \leq g(x) \leq g^U \\
                &&x^L \leq x \leq x^U, \label{bounds}
\end{eqnarray}
where $x \in \RR^n$ are the optimization variables (possibly with
lower and upper bounds, $x^L\in(\RR\cup\{-\infty\})^n$ and
$x^U\in(\RR\cup\{+\infty\})^n$), $f:\RR^n\longrightarrow\RR$ is the
objective function, and $g:\RR^n\longrightarrow \RR^m$ are the general
nonlinear constraints.  The functions $f(x)$ and $g(x)$ can be linear
or nonlinear and convex or non-convex (but are assumed to be twice
continuously differentiable). The constraints, $g(x)$, have lower and
upper bounds, $g^L\in(\RR\cup\{-\infty\})^n$ and
$g^U\in(\RR\cup\{+\infty\})^m$. Note that equality constraints of the
form $g_i(x)=\bar g_i$ can be specified by setting $g^L_{i}=g^U_{i}=\bar g_i$.

Ipopt implements an interior point line search filter method. The
mathematical details of the algorithm can be found in the reports
\cite{AndreasPaper} \cite{AndreasThesis}.

The Ipopt package is available from COIN-OR\cite{COINORWeb} under the
open-source CPL license and includes the complete source code for
Ipopt. The Ipopt distribution generates an executable for different
modeling environments, including AMPL. As well, you can link your
problem statement with Ipopt using interfaces for Fortran, C, or
C++. Ipopt can be used with most Linux/Unix environments, and on
Windows using Visual Studio .NET or Cygwin.  The purpose of this
document is to demonstrate how to solve problems using Ipopt. This
includes installation and compilation of Ipopt for use with AMPL as
well as linking with your own code. General questions related to Ipopt
should be addressed to Ipopt mailing list ({\tt
http://list.coin-or.org/mailman/listinfo/coin-ipopt}). You might want
to look at the archives before posting a question.

\section{History of Ipopt}
The original Ipopt (Fortran version) was a product of the dissertation
research of Andreas W\"achter \cite{AndreasThesis}, under Lorenz
T. Biegler at the Chemical Engineering Department at Carnegie Mellon
University. The code was made open source and distributed by the
COIN-OR initiative, which is now a non-profit corporation.  Ipopt has
been actively developed under COIN-OR since 2002.

To continue natural extension of the code and allow easy addition of
new features, IBM Research decided to invest in an open source
re-write of Ipopt in C++.  The new C++ version of the Ipopt
optimization code (Ipopt 3.0 and beyond) is currently developed at IBM
Research and remains part of the COIN-OR initiative. Future
development on the Fortran version will cease with the exception of
occasional bug fix releases.

This document is a guide to using Ipopt 3.0 (the new C++ version of Ipopt).

%\section{Interfacing to Ipopt}

\section{Getting the Code}
Ipopt is available from the COIN-OR subversion repository. You can
either download the code using svn (the subversion client similar to
cvs) or simply retrieve the tarball (at the writing of this document,
the tarballs were not available, therefore, directions only include
use of svn).  While the tarball is an easy method to retrieve the
code, using the subversion system allows users the benefits of the
version control system, including easy updates and revision
control.

To use subversion, follow the steps below:
\begin{enumerate}
\item{Create a directory to store the code}\\
{\tt \$ mkdir Ipopt}\\ 
Note the {\tt \$} indicates the command line
prompt, do not type {\tt \$}, only the text following it.
\item{Download the code to the new directory}\\
{\tt \$ cd Ipopt; svn co https://www.coin-or.org/svn/ipopt-devel/trunk}
\end{enumerate}

\subsection{Download External Code}
Ipopt uses a few external packages that are not included in the
distribution, namely ASL (the Ampl Solver Library), BLAS, and
some routines from the Harwell Subroutine Library.

Note that you only need to obtain the ASL if you intend to use Ipopt
from AMPL.  It is not required if you want to specify yout
optimization problem in a programming language (C++, C, or Fortran).

\subsubsection{Download BLAS and ASL}
If you have the download utility \texttt{wget} installed on your
system, retrieving ASL, and BLAS is straightforward using scripts
included with the ipopt distribution. These scripts download the
required files from the Netlib Repository (http://www.netlib.org).\\

\noindent
{\tt \$ cd trunk/ipopt/Extern/blas; ./get.blas}\\
{\tt \$ cd ../ASL; ./get.asl}\\

\noindent
If you don't have \texttt{wget} installed on your system, please read
the \texttt{INSTALL.*} files in the \texttt{trunk/ipopt/Extern/blas}
and \texttt{trunk/ipopt/Extern/ASL} directories for alternative
instructions.

\subsubsection{Download HSL Subroutines}
In addition to the IPOPT source code, two additional subroutines have
to be downloaded from the Harwell Subroutine Library (HSL).  The
required routines are freely available for non-commercial, academic
use, but it is your responsibility to investigate the licensing of all
third party code.
\begin{enumerate}
\item{Go to {\tt http://hsl.rl.ac.uk/archive/hslarchive.html}}
\item{Follow the instruction on the website, and submit the registration form.}
\item{Go to \textit{HSL Archive Programs}, and find the package list.}
\item{In your browser window, click on \textit{MA27}.}
\item{Make sure that \textit{Double precision:} is checked. 
Click \textit{Download package (comments removed)}}
\item{Save the file as {\tt ma27ad.f} in {\tt ipopt/trunk/Extern/HSL/}}\\
Note: Some browsers append a file extension ({\tt .txt}) when you save
the file, so you may have to rename it.
\item{Go back to the package list using the back button of your browser.}
\item{In your browser window, click on \textit{MC19}.}
\item{Make sure \textit{Double precision:} is checked. Click 
\textit{Download package (comments removed)}}
\item{Save the file as {\tt mc19ad.f} in {\tt
ipopt/trunk/Extern/HSL/}}\\
Note: Some browsers append a file extension ({\tt .txt}) when you save
the file, so you may have to rename it.
\end{enumerate}


\section{Compiling and Installing Ipopt} \label{sec.comp_and_inst}
Ipopt can be easily compiled and installed with the usual {\tt configure},
{\tt make}, {\tt make install} commands.
\begin{enumerate}
\item{Go to the main directory of Ipopt:\\
{\tt \$ cd; cd ipopt/trunk}}
\item{Run the configure script}\\
{\tt \$ ./configure}\\ 
The default configure (without any options) is
sufficient for most users. If you want to see the configure options,
run {\tt ./configure --help}.
\item{Build the code}\\
{\tt \$ make}
\item{Install Ipopt}\\
{\tt \$ make install}\\ 
This installs the the ipopt library and
necessary header files to ?...?
\item{Test the installation}\\
{\tt \$ make test}\\
This should ?...?
\end{enumerate}

\section{Interfacing your NLP to Ipopt: A tutorial example.}
Ipopt has been designed to be flexible for a wide variety of
applications, and there are a number of ways to interface with Ipopt
that allow specific data structures and linear solver
techniques. Nevertheless, the authors have included a standard
representation that should meet the needs of most users.

This tutorial will discuss four interfaces to Ipopt, namely the AMPL
modeling language interface, and the C, C++, and Fortran code
interfaces.  AMPL is a 3rd party modeling language tool that allows
users to write their optimization problem in a syntax that resembles
the way the problem would be written mathematically. Once the problem
has been formulated in AMPL, the problem can be easily solved using
the (already compiled) executable. Interfacing your problem by
directly linking code requires more effort to write, but can be far
more efficient for large problems.

We will illustrate how to use each of the four interfaces using an
example problem, number 71 from the Hock-Schittkowsky test suite,
\begin{eqnarray}
\min_{x \in \Re^4} &x_1 x_4 (x_1 + x_2 + x_3)  +  x_3 \label{ex_obj} \\
\mbox{s.t.} \;  &x_1 x_2 x_3 x_4 \ge 25 \label{ex_ineq} \\
                &x_1^2 + x_2^2 + x_3^2 + x_4^2  =  40 \label{ex_equ} \\
                &1 \leq x_1, x_2, x_3, x_4 \leq 5, \label{ex_bounds}
\end{eqnarray}
with the starting point,
\begin{equation}
x {=} (1, 5, 5, 1) \label{ex_startpt}
\end{equation}
and the optimal solution,
\[
x^\star {=} (1.00000000, 4.74299963, 3.82114998, 1.37940829). \nonumber
\]

\section{Tutorial Example: Using the AMPL interface}
Interfacing through the AMPL interface is by far the easiest way to
solve a problem with Ipopt. The user must simply formulate the problem
in AMPL syntax, and solve the problem through the AMPL environment.
There are drawbacks, however. AMPL is a 3rd party package and, as
such, must be appropriately licensed (a free, student version for
limited problem size is available from the AMPL website,
www.ampl.com). Furthermore, the AMPL environment may be prohibitive
for very large problems. Nevertheless, formulating the problem in AMPL
is straightforward and even for large problems, it is often used as a
prototyping tool before using one of the code interfaces.

This tutorial is not intended as a guide to formulating models in
AMPL. If you are not already familiar with AMPL, please consult
\cite{AMPL_REFERENCE}.

The problem presented in equations (\ref{ex_obj}-\ref{ex_startpt}) can
be solved with Ipopt with the following AMPL mod file.
\subsubsection{hs071\_ampl.mod}
\begin{verbatim}
# tell ampl to use the ipopt executable as a solver
# make sure ipopt is in the path!
option solver ipopt;

# declare the variables and their bounds, 
# set notation could be used, but this is straightforward
var x1 >= 1, <= 5; 
var x2 >= 1, <= 5; 
var x3 >= 1, <= 5; 
var x4 >= 1, <= 5;

# specify the objective function
minimize obj:
                x1 * x4 * (x1 + x2 + x3) + x3;
        
# specify the constraints
s.t.
        
        inequality:
                x1 * x2 * x3 * x4 >= 25;
                
        equality:
                x1^2 + x2^2 + x3^2 +x4^2 = 40;

# specify the starting point            
let x1 := 1;
let x2 := 5;
let x3 := 5;
let x4 := 1;

# solve the problem
solve;

# print the solution
display x1;
display x2;
display x3;
display x4;
\end{verbatim}

The line, "{\tt option solver ipopt;}" tells ampl to use ipopt as the
solver. The ipopt executable (installed in Section
{\ref{sec.comp_and_inst}) must be in the path for AMPL to find it. The
remaining lines specify the problem in AMPL format. The problem can
now be solved by starting ampl and loading the mod file.
\begin{verbatim}
$ ampl
> model hs071_ampl.mod;
.
.
.
\end{verbatim}
%$
The problem will be solved using Ipopt and the solution will be
displayed.

At this point, AMPL users may wish to skip the sections about
interfacing with code, but should read Section \ref{sec.options}
concerning Ipopt options, and Section \ref{sec.ipopt_output} which
explains the output displayed by ipopt.

\section{Tutorial Example: Interfacing with Ipopt through code}
In order to solve a problem, Ipopt needs more information than just
the problem definition (for example, the derivative information). If
you are using a modeling language like AMPL, the extra information is
provided by the modeling tool and the Ipopt interface. When
interfacing with Ipopt through your own code, however, you must
provide this additional information.

\begin{figure}
\caption{Information Required By Ipopt}
\begin{enumerate}
        \item Problem dimensions \label{it.prob_dim}
                \begin{itemize}
                        \item number of variables
                        \item number of constraints
                \end{itemize}
        \item Problem bounds
                \begin{itemize}
                        \item variable bounds
                        \item constraint bounds
                \end{itemize}
        \item Initial starting point
                \begin{itemize}
                        \item Initial values for the primal $x$ variables
                        \item Initial values for the multipliers (only
                          required for a warm start option)
                \end{itemize}
        \item Problem Structure \label{it.prob_struct}
                \begin{itemize}
                \item number of nonzeros in the Jacobian of the constraints
                \item number of nonzeros in the Hessian of the Lagrangian
                \item Structure of the Jacobian of the constraints
                \item Structure of the Hessian of the Lagrangian
                \end{itemize}
        \item Evaluation of Problem Functions \label{it.prob_eval} \\
        Information evaluated using a given point 
        ($x_k, \lambda_k, \sigma_f$ coming from Ipopt)
                \begin{itemize}
                        \item Objective function, $f(x_k)$
                        \item Gradient of the objective $\nabla f(x_k)$
                        \item Constraint residuals, $g(x_k)$
                        \item Jacobian of the constraints, $\nabla g(x_k)$
                        \item Hessian of the Lagrangian, 
                        $\sigma_f \nabla^2 f(x_k) + \sum_{i=1}^m\lambda_i\nabla^2 g_i(x_k)$ 
                \end{itemize}
\end{enumerate}
\label{fig.required_info}
\end{figure}
\vspace{0.1in}
The information required by Ipopt is shown in Figure
\ref{fig.required_info}. The problem dimensions and bounds are
straightforward and come solely from the problem definition. The
initial starting point is used by the algorithm when it begins
iterating to solve the problem. If Ipopt has difficulty converging, or
if it converges to a locally infeasible point, adjusting the starting
point may help.

Providing the problem structure is a bit more involved. Ipopt is a
nonlinear programming solver that is designed for solving large scale,
sparse problems. While Ipopt can be customized for a variety of matrix
formats, a triplet format is used for the standard interfaces in this
tutorial. For an overview of the triplet format for sparse matrices,
see Appendix \ref{app.triplet}. Before solving the problem, Ipopt
needs to know the number of nonzeros and the structure (row and column
indices of each of the nonzeros) of the Jacobian and the Hessian. Once
defined, this nonzero structure MUST remain constant for the entire
problem. This means that the structure needs to include entries for
any element that could ever be nonzero, not only those that are
nonzero at the starting point.

As Ipopt iterates, it will need the values for the items in
(\ref{it.prob_eval}) evaluated at particular points. Before we can
begin coding the interface, however, we need to work out the details
of these equations symbolically for example problem
(\ref{ex_obj}-\ref{ex_bounds}).

The gradient of the objective is given by
\begin{equation}
\left[
\begin{array}{c}
x_1 x_4 + x_4 (x_1 + x_2 + x_3) \\
x_1 x_4 \\
x_1 x_4 + 1 \\
x_1 (x_1 + x_2 + x_3)
\end{array}
\right],
\end{equation}
the Jacobian of 
the constraints is,
\begin{equation}
\left[
\begin{array}{cccc}
x_2 x_3 x_4     & x_1 x_3 x_4   & x_1 x_2 x_4   & x_1 x_2 x_3   \\
2 x_1           & 2 x_2         & 2 x_3         & 2 x_4
\end{array}
\right].
\end{equation}

We need to determine the Hessian of the Lagrangian.  The Lagrangian is
given by $f(x) + g(x)^T \lambda$ and the Hessian of the Lagrangian is
technically, $ \nabla^2 f(x_k) + sum_{i=1}^m\lambda_i\nabla^2 g_i(x_k)$,
however, so that Ipopt can ask for the Hessian of the objective or the
constraints independently if required, we introduce a factor
($\sigma_f$) in front of the objective term. The value for $\sigma_f$
is generally $1$, although it may include scaling factors or even be
set to zero to retrieve the Hessian of the constraints alone.

For our implementation then, the symbolic form of the Hessian of the
Lagrangian (with the $\sigma_f$ parameter) is,
%\begin{eqnarray}
%{\cal L}(x,\lambda) &{=}& f(x) + c(x)^T \lambda \nonumber \\
%&{=}& \left(x_1 x_4 (x_1 + x_2 + x_3)  +  x_3\right) 
%+ \left(x_1 x_2 x_3 x_4\right) \lambda_1 \nonumber \\
%&& \;\;\;\;\;+ \left(x_1^2 + x_2^2 + x_3^2 + x_4^2\right) \lambda_2 
%- \displaystyle \sum_{i \in 1..4} z^L_i + \sum_{i \in 1..4} z^U_i
%\end{eqnarray}
\begin{equation}
\scriptsize
\sigma_f \left[
\begin{array}{cccc}
2 x_4           & x_4           & x_4           & 2 x_1 + x_2 + x_3     \\
x_4             & 0             & 0             & x_1                   \\
x_4             & 0             & 0             & x_1                   \\
2 x_1+x_2+x_3   & x_1           & x_1           & 0
\end{array}
\right]
+
\lambda_1
\left[
\begin{array}{cccc}
0               & x_3 x_4       & x_2 x_4       & x_2 x_3       \\
x_3 x_4         & 0             & x_1 x_4       & x_1 x_3       \\
x_2 x_4         & x_1 x_4       & 0             & x_1 x_2       \\
x_2 x_3         & x_1 x_3       & x_1 x_2       & 0 
\end{array}
\right]
+
\lambda_2
\left[
\begin{array}{cccc}
2       & 0     & 0     & 0     \\
0       & 2     & 0     & 0     \\
0       & 0     & 2     & 0     \\
0       & 0     & 0     & 2
\end{array}
\right]
\end{equation}
where the first term comes from the Hessian of the objective function, and the 
second and third term from the Hessian of (\ref{ex_ineq}) and (\ref{ex_equ})
respectively. Therefore, the dual variables $\lambda_1$ and $\lambda_2$ 
are then the multipliers for constraints (\ref{ex_ineq}) and (\ref{ex_equ})
respectively.

%C =============================================================================
%C
%C     This is an example for the usage of IPOPT.
%C     It implements problem 71 from the Hock-Schittkowsky test suite:
%C
%C     min   x1*x4*(x1 + x2 + x3)  +  x3
%C     s.t.  x1*x2*x3*x4                   >=  25
%C           x1**2 + x2**2 + x3**2 + x4**2  =  40
%C           1 <=  x1,x2,x3,x4  <= 5
%C
%C     Starting point:
%C        x = (1, 5, 5, 1)
%C
%C     Optimal solution:
%C        x = (1.00000000, 4.74299963, 3.82114998, 1.37940829)
%C
%C =============================================================================

The remainder of this section of the tutorial will lead you through
the coding required to solve example problem
(\ref{ex_obj}-\ref{ex_bounds}) using, first C++, then C, and finally
Fortran. Completed versions of these examples can be found in {\tt
Ipopt/trunk/Examples} under {\tt hs071\_cpp}, {\tt hs071\_c}, {\tt
hs071\_f}.

As a user, you are responsible for coding two sections of the program
that solves a problem using Ipopt: the executable ({\tt main}) and the
problem representation.  Generally, you will write an executable that
prepares the problem, and then passes control over to Ipopt through an
{\tt Optimize} call. In this {\tt Optimize} call, you will give Ipopt
everything that it requires to call back to your code whenever it
needs functions evaluated (like the objective, the Jacobian, etc.).
In each of the three sections that follow (C++, C, and Fortran), we
will first discuss how to code the problem representation, and then
how to code the executable.

\subsection{The C++ Interface}
This tutorial assumes that you are familiar with the C++ programming
language, however, we will lead you through each step of the
implementation. For the problem representation, we will create a class
that inherits off of the pure virtual base class, {\tt TNLP}
({\tt IpTNLP.hpp}). For the executable (the {\tt main} function) we will
make the call to Ipopt through the IpoptApplication class
(IpIpoptApplication.hpp). In addition, we will also be using the
SmartPtr class ({\tt IpSmartPtr.hpp}) which implements a reference counting
pointer that takes care of memory management (object deletion) for
you.

\subsubsection{Coding the Problem Representation}\label{sec.cpp_problem}
We provide the information required in Figure \ref{fig.required_info}
by coding the {\tt HS071\_NLP} class, a specific implementation of the
{\tt TNLP} base class. In the executable, we will create an instance
of the {\tt HS071\_NLP} class and give this class to Ipopt so it can
evaluate the problem functions through the {\tt TNLP} interface. If
you have any difficulty as the implementation proceeds, have a look at
the completed example in the {\tt hs071\_cpp} directory.

Start by creating a new directory under Examples, called MyExample and
create the files {\tt hs071\_nlp.hpp} and {\tt hs071\_nlp.cpp}. In
{\tt hs071\_nlp.hpp}, include {\tt IpTNLP.hpp} (the base class), tell
the compiler that we are using the Ipopt namespace, and create the
declaration of the {\tt HS071\_NLP} class, inheriting off of {\tt
  TNLP}. Have a look at the {\tt TNLP} class in {\tt IpTNLP.hpp}; you
will see eight pure virtual methods that we must implement. Declare
these methods in the header file.  Implement each of the methods in
{\tt HS071\_NLP.cpp} using the descriptions given below. In {\tt
  hs071\_nlp.cpp}, first include the header file for your class and
tell the compiler that you are using the Ipopt namespace. A full
version of these files can be found in the {\tt Examples/hs071\_cpp}
directory.

\paragraph{virtual bool get\_nlp\_info(Index\& n, Index\& m, Index\& nnz\_jac\_g, \\
Index\& nnz\_h\_lag, IndexStyleEnum\& index\_style)} 
$\;$ \\
Give Ipopt the information about the size of the problem (and hence,
the size of the arrays that it needs to allocate). 
\begin{itemize}
\item {\tt n}: (out), the number of variables in the problem (dimension of {\tt x}).
\item {\tt m}: (out), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt nnz\_jac\_g}: (out), the number of nonzero entries in the Jacobian.
\item {\tt nnz\_h\_lag}: (out), the number of nonzero entries in the Hessian.
\item {\tt index\_style}: (out), the style used for row/col entries in the sparse matrix
format (C\_STYLE: 0-based, FORTRAN\_STYLE: 1-based).
\end{itemize}
Ipopt uses this information when allocating the arrays that
it will later ask you to fill with values. Be careful in this method
since incorrect values will cause memory bugs which may be very
difficult to find.

Our example problem has 4 variables (n), and two constraints (m). The
Jacobian for this small problem is actually dense and has 8 nonzeros
(we will still represent this Jacobian using the sparse matrix triplet
format). The Hessian of the Lagrangian has 10 ``symmetric'' nonzeros.
Keep in mind that the number of nonzeros is the total number of
elements that may \emph{ever} be nonzero, not just those that are
nonzero at the starting point. This information is set once for the
entire problem.

\begin{verbatim}
bool HS071_NLP::get_nlp_info(Index& n, Index& m, Index& nnz_jac_g, 
                             Index& nnz_h_lag, IndexStyleEnum& index_style)
{
  // The problem described in HS071_NLP.hpp has 4 variables, x[0] through x[3]
  n = 4;

  // one equality constraint and one inequality constraint
  m = 2;

  // in this example the Jacobian is dense and contains 8 nonzeros
  nnz_jac_g = 8;

  // the Hessian is also dense and has 16 total nonzeros, but we
  // only need the lower left corner (since it is symmetric)
  nnz_h_lag = 10;

  // use the C style indexing (0-based)
  index_style = TNLP::C_STYLE;

  return true;
}
\end{verbatim}

\paragraph{virtual bool get\_bounds\_info(Index n, Number* x\_l, Number* x\_u, \\
                            Index m, Number* g\_l, Number* g\_u)} 
$\;$ \\
Give Ipopt the value of the bounds on the variables and constraints.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x\_l}: (out) the lower bounds for {\tt x}. 
\item {\tt x\_u}: (out) the upper bounds for {\tt x}.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt g\_l}: (out) the lower bounds for {\tt g}. 
\item {\tt g\_u}: (out) the upper bounds for {\tt g}.
\end{itemize}
The values of {\tt n} and {\tt m} that you specified in {\tt
  get\_nlp\_info} are passed to you for debug checking.  Setting a
lower bound to a value less than or equal to the value of the option
{\tt nlp\_lower\_bound\_inf} (data member of {\tt TNLP}) will cause
Ipopt to assume no lower bound. Likewise, specifying the upper bound
above or equal to the value of the option {\tt nlp\_upper\_bound\_inf}
will cause Ipopt to assume no upper bound.  These options, {\tt
  nlp\_lower\_bound\_inf} and {\tt nlp\_upper\_bound\_inf}, are set to
$-10^{19}$ and $10^{19}$ respectively, by default, but may be modified
by changing the options (see Section \ref{sec.options}).

In our example, the first constraint has a lower bound of $25$ and no upper
bound, so we set the lower bound of constraint {\tt [0]} to $25$ and
the upper bound to some number greater than $10^{19}$. The second
constraint is an equality constraint and we set both bounds to
$40$. Ipopt recognizes this as an equality constraint and does not
treat it as two inequalities.

\begin{verbatim}
bool HS071_NLP::get_bounds_info(Index n, Number* x_l, Number* x_u,
                            Index m, Number* g_l, Number* g_u)
{
  // here, the n and m we gave IPOPT in get_nlp_info are passed back to us.
  // If desired, we could assert to make sure they are what we think they are.
  assert(n == 4);
  assert(m == 2);

  // the variables have lower bounds of 1
  for (Index i=0; i<4; i++) {
    x_l[i] = 1.0;
  }

  // the variables have upper bounds of 5
  for (Index i=0; i<4; i++) {
    x_u[i] = 5.0;
  }

  // the first constraint g1 has a lower bound of 25
  g_l[0] = 25;
  // the first constraint g1 has NO upper bound, here we set it to 2e19.
  // Ipopt interprets any number greater than nlp_upper_bound_inf as 
  // infinity. The default value of nlp_upper_bound_inf and nlp_lower_bound_inf
  // is 1e19 and can be changed through ipopt options.
  g_u[0] = 2e19;

  // the second constraint g2 is an equality constraint, so we set the 
  // upper and lower bound to the same value
  g_l[1] = g_u[1] = 40.0;

  return true;
}
\end{verbatim}

\paragraph{virtual bool get\_starting\_point(Index n, bool init\_x, Number* x, \\
              bool init\_z, Number* z\_L, Number* z\_U, Index m, 
        bool init\_lambda, Number* lambda)} 
$\;$ \\
Give Ipopt the starting point before it begins iterating.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt init\_x}: (in), if true, this method must provide an initial value for {\tt x}.
\item {\tt x}: (out), the initial values for the primal variables, {\tt x}.
\item {\tt init\_z}: (in), if true, this method must provide an initial value 
        for the bound multipliers {\tt z\_L}, and {\tt z\_U}.
\item {\tt z\_L}: (out), the initial values for the bound multipliers, {\tt z\_L}.
\item {\tt z\_U}: (out), the initial values for the bound multipliers, {\tt z\_U}.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt init\_lambda}: (in), if true, this method must provide an initial value 
        for the constraint multipliers, {\tt lambda}.
\item {\tt lambda}: (out), the initial values for the constraint multipliers, {\tt lambda}.
\end{itemize}

The index variables $n$, and $m$ are passed in only to help your
debugging. These variables will have the same values you specified in
{\tt get\_nlp\_info}.

Depending on the options that have been set,
Ipopt may or may not require bounds for the primal variables $x$, the
bound multipliers $z$, and the equality multipliers $\lambda$. The
boolean flags {\tt init\_x},{\tt init\_z}, and {\tt init\_lambda} tell
you whether or not you should provide initial values for $x$, $z$, or
$\lambda$ respectively. The default options only require an
initial value for the primal variables {\tt x}.

In our example, we provide initial values for {\tt x} as specified in the example
problem. We do not provide any initial values for the dual variables,
but use an assert to immediately let us know if we are ever asked for
them.

\begin{verbatim}
bool HS071_NLP::get_starting_point(Index n, bool init_x, Number* x,
                               bool init_z, Number* z_L, Number* z_U,
                               Index m, bool init_lambda,
                               Number* lambda)
{
  // Here, we assume we only have starting values for x, if you code
  // your own NLP, you can provide starting values for the dual variables
  // if you wish to use a warmstart option
  assert(init_x == true);
  assert(init_z == false);
  assert(init_lambda == false);

  // initialize to the given starting point
  x[0] = 1.0;
  x[1] = 5.0;
  x[2] = 5.0;
  x[3] = 1.0;

  return true;
}
\end{verbatim}

\paragraph{virtual bool eval\_f(Index n, const Number* x, 
                bool new\_x, Number\& obj\_value)} 
$\;$ \\
Return the value of the objective function as calculated using {\tt x}.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt new\_x}: (in), false if any evaluation method was previously called 
        with the same values in {\tt x}, true otherwise.
\item {\tt obj\_value}: (out) the value of the objective function ($f(x)$).
\end{itemize}

The boolean variable {\tt new\_x} will be false if the last call to
any of the evaluation methods used the same $x$ values. This can be
helpful when users have efficient implementations that calculate
multiple outputs at once. Ipopt internally caches results from the
{\tt TNLP} and generally, this flag can be ignored.

The index variable $n$ is passed in only to help your debugging. This
variable will have the same value you specified in {\tt
get\_nlp\_info}.

For our example, we ignore the {\tt new\_x} flag and calculate the objective.

\begin{verbatim}
bool HS071_NLP::eval_f(Index n, const Number* x, bool new_x, Number& obj_value)
{
  assert(n == 4);

  obj_value = x[0] * x[3] * (x[0] + x[1] + x[2]) + x[2];

  return true;
}
\end{verbatim}

\paragraph{virtual bool eval\_grad\_f(Index n, const Number* x, bool new\_x, 
        Number* grad\_f)} 
$\;$ \\
Return the gradient of the objective to Ipopt, as calculated by the values in {\tt x}.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt new\_x}: (in), false if any evaluation method was previously called 
        with the same values in {\tt x}, true otherwise.
\item {\tt grad\_f}: (out) the array of values for the gradient of the 
        objective function ($\nabla f(x)$).
\end{itemize}

The gradient array is in the same order as the $x$ variables (i.e.\ the
gradient of the objective with respect to {\tt x[2]} should be put in
{\tt grad\_f[2]}).

The boolean variable {\tt new\_x} will be false if the last call to
any of the evaluation methods used the same $x$ values. This can be
helpful when users have efficient implementations that caclulate
multiple outputs at once. Ipopt internally caches results from the
{\tt TNLP} and generally, this flag can be ignored.

The index variable $n$ is passed in only to help your debugging. This
variable will have the same value you specified in {\tt
get\_nlp\_info}.

In our example, we ignore the {\tt new\_x} flag and calculate the
values for the gradient of the objective.
\begin{verbatim}
bool HS071_NLP::eval_grad_f(Index n, const Number* x, bool new_x, Number* grad_f)
{
  assert(n == 4);

  grad_f[0] = x[0] * x[3] + x[3] * (x[0] + x[1] + x[2]);
  grad_f[1] = x[0] * x[3];
  grad_f[2] = x[0] * x[3] + 1;
  grad_f[3] = x[0] * (x[0] + x[1] + x[2]);

  return true;
}
\end{verbatim}


\paragraph{virtual bool eval\_g(Index n, const Number* x, 
        bool new\_x, Index m, Number* g)}
$\;$ \\
Give Ipopt the value of the constraints as calculated by the values in {\tt x}.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt new\_x}: (in), false if any evaluation method was previously called 
        with the same values in {\tt x}, true otherwise.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt g}: (out) the array of constraint residuals.
\end{itemize}

The values returned in {\tt g} should be only the $g(x)$ values, 
do not add or subtract the bound values $g_l$ or $g_u$.

The boolean variable {\tt new\_x} will be false if the last call to
any of the evaluation methods used the same $x$ values. This can be
helpful when users have efficient implementations that calculate
multiple outputs at once. Ipopt internally caches results from the
{\tt TNLP} and generally, this flag can be ignored.

The index variables $n$, and $m$ are passed in only to help your
debugging. These variables will have the same values you specified in
{\tt get\_nlp\_info}.

In our example, we ignore the {\tt new\_x} flag and calculate the
values for the gradient of the objective.
\begin{verbatim}
bool HS071_NLP::eval_g(Index n, const Number* x, bool new_x, Index m, Number* g)
{
  assert(n == 4);
  assert(m == 2);

  g[0] = x[0] * x[1] * x[2] * x[3];
  g[1] = x[0]*x[0] + x[1]*x[1] + x[2]*x[2] + x[3]*x[3];

  return true;
} 
\end{verbatim}

\paragraph{virtual bool eval\_jac\_g(Index n, const Number* x, bool new\_x, \\
                       Index m, Index nele\_jac, Index* iRow, 
        Index *jCol, Number* values)}
$\;$ \\
Return either the structure of the Jacobian of the constraints, or the values for the 
Jacobian of the constraints as calculated by the values in {\tt x}.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt new\_x}: (in), false if any evaluation method was previously called 
        with the same values in {\tt x}, true otherwise.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt n\_elel\_jac}: (in), the number of nonzero elements in the 
        Jacobian (dimension of {\tt iRow}, {\tt jCol}, and {\tt values}).
\item {\tt iRow}: (out), the row indices of entries in the Jacobian of the constraints.
\item {\tt jCol}: (out), the column indices of entries in the Jacobian of the constraints.
\item {\tt values}: (out), the values of the entries in the Jacobian of the constraints.
\end{itemize}

The Jacobian is the matrix of derivatives where
the derivative of constraint $i$ with respect to variable $j$ is
placed in row $i$ and column $j$. See Appendix \ref{app.triplet} for a discussion of 
the sparse matrix format used in this method.

If the {\tt iRow} and {\tt jCol} arguments are not NULL, then Ipopt
wants you to fill in the structure of the Jacobian (the row and column
indices only). At this time, the {\tt x} argument and the {\tt values}
argument will be NULL.

If the {\tt x} argument and the {\tt values} argument are not NULL,
then Ipopt wants you to fill in the values of the Jacobian as
calculated from the array {\tt x} (using the same order as you used
when specifying the structure). At this time, the {\tt iRow} and {\tt
jCol} arguments will be NULL;

The boolean variable {\tt new\_x} will be false if the last call to
any of the evaluation methods used the same $x$ values. This can be
helpful when users have efficient implementations that caclulate
multiple outputs at once. Ipopt internally caches results from the
{\tt TNLP} and generally, this flag can be ignored.

The index variables {\tt n}, {\tt m}, and {\tt nele\_jac} are passed
in only to help your debugging. These arguments will have the same
values you specified in {\tt get\_nlp\_info}.

In our example, the Jacobian is actually dense, but we still
specify it using the sparse format.

\begin{verbatim}
bool HS071_NLP::eval_jac_g(Index n, const Number* x, bool new_x,
                       Index m, Index nele_jac, Index* iRow, Index *jCol,
                       Number* values)
{
  if (values == NULL) {
    // return the structure of the Jacobian

    // this particular Jacobian is dense
    iRow[0] = 0; jCol[0] = 0;
    iRow[1] = 0; jCol[1] = 1;
    iRow[2] = 0; jCol[2] = 2;
    iRow[3] = 0; jCol[3] = 3;
    iRow[4] = 1; jCol[4] = 0;
    iRow[5] = 1; jCol[5] = 1;
    iRow[6] = 1; jCol[6] = 2;
    iRow[7] = 1; jCol[7] = 3;
  }
  else {
    // return the values of the Jacobian of the constraints
    
    values[0] = x[1]*x[2]*x[3]; // 0,0
    values[1] = x[0]*x[2]*x[3]; // 0,1
    values[2] = x[0]*x[1]*x[3]; // 0,2
    values[3] = x[0]*x[1]*x[2]; // 0,3

    values[4] = 2*x[0]; // 1,0
    values[5] = 2*x[1]; // 1,1
    values[6] = 2*x[2]; // 1,2
    values[7] = 2*x[3]; // 1,3
  }

  return true;
}
\end{verbatim}

\paragraph{virtual bool eval\_h(Index n, const Number* x, bool new\_x,\\
   Number obj\_factor, Index m, const Number* lambda, bool new\_lambda,\\
        Index nele\_hess, Index* iRow, Index* jCol, Number* values)}
$\;$ \\
Return the structure of the Hessian of the Lagrangian or the values of the 
Hessian of the Lagrangian as calculated by the values in {\tt obj\_factor},
{\tt x}, and {\tt lambda}.
\begin{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt new\_x}: (in), false if any evaluation method was previously called 
        with the same values in {\tt x}, true otherwise.
\item {\tt obj\_factor}: (in), factor in front of the objective term in the Hessian.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt lambda}: (in), the current values of the equality multipliers to use
        for each constraint in the evaluation of the Hessian.
\item {\tt n\_elel\_jac}: (in), the number of nonzero elements in the 
        Jacobian (dimension of {\tt iRow}, {\tt jCol}, and {\tt values}).
\item {\tt new\_lambda}: (in), false if any evaluation method was previously called 
        with the same values in {\tt lambda}, true otherwise.
\item {\tt nele\_hess}: (in), the number of nonzero elements in the Hessian 
        (dimension of {\tt iRow}, {\tt jCol}, and {\tt values}.
\item {\tt iRow}: (out), the row indices of entries in the Hessian.
\item {\tt jCol}: (out), the column indices of entries in the Hessian.
\item {\tt values}: (out), the values of the entries in the Hessian.
\end{itemize}

If the {\tt iRow} and {\tt jCol} arguments are not NULL, then Ipopt
wants you to fill in the structure of the Hessian (the row and column
indices only). In this case, the {\tt x}, {\tt lambda}, and {\tt
values} arrays will be NULL.

If the {\tt x}, {\tt lambda}, and {\tt values} arrays are not NULL,
then Ipopt wants you to fill in the values of the Hessian as
calculated using {\tt x} and {\tt lambda} (using the same order as you
used when specifying the structure). In this case, the {\tt iRow} and
{\tt jCol} arguments will be NULL.

The boolean variables {\tt new\_x} and {\tt new\_lambda} will both be
false if the last call to any of the evaluation methods used the same
values. This can be helpful when users have efficient implementations
that caclulate multiple outputs at once. Ipopt internally caches
results from the {\tt TNLP} and generally, this flag can be ignored.

The index variables {\tt n}, {\tt m}, and {\tt nele\_hess} are passed
in only to help your debugging. These arguments will have the same
values you specified in {\tt get\_nlp\_info}.

In our example, the Hessian is dense, but we still specify it using the
sparse matrix format. Because the Hessian is symmetric, we only need to 
specify the lower left corner.
\begin{verbatim}
bool HS071_NLP::eval_h(Index n, const Number* x, bool new_x,
                   Number obj_factor, Index m, const Number* lambda,
                   bool new_lambda, Index nele_hess, Index* iRow,
                   Index* jCol, Number* values)
{
  if (values == NULL) {
    // return the structure. This is a symmetric matrix, fill the lower left
    // triangle only.

    // the Hessian for this problem is actually dense
    Index idx=0;
    for (Index row = 0; row < 4; row++) {
      for (Index col = 0; col <= row; col++) {
        iRow[idx] = row; 
        jCol[idx] = col;
        idx++;
      }
    }
    
    assert(idx == nele_hess);
  }
  else {
    // return the values. This is a symmetric matrix, fill the lower left
    // triangle only

    // fill the objective portion
    values[0] = obj_factor * (2*x[3]); // 0,0

    values[1] = obj_factor * (x[3]);   // 1,0
    values[2] = 0;                     // 1,1

    values[3] = obj_factor * (x[3]);   // 2,0
    values[4] = 0;                     // 2,1
    values[5] = 0;                     // 2,2

    values[6] = obj_factor * (2*x[0] + x[1] + x[2]); // 3,0
    values[7] = obj_factor * (x[0]);                 // 3,1
    values[8] = obj_factor * (x[0]);                 // 3,2
    values[9] = 0;                                   // 3,3


    // add the portion for the first constraint
    values[1] += lambda[0] * (x[2] * x[3]); // 1,0
    
    values[3] += lambda[0] * (x[1] * x[3]); // 2,0
    values[4] += lambda[0] * (x[0] * x[3]); // 2,1

    values[6] += lambda[0] * (x[1] * x[2]); // 3,0
    values[7] += lambda[0] * (x[0] * x[2]); // 3,1
    values[8] += lambda[0] * (x[0] * x[1]); // 3,2

    // add the portion for the second constraint
    values[0] += lambda[1] * 2; // 0,0

    values[2] += lambda[1] * 2; // 1,1

    values[5] += lambda[1] * 2; // 2,2

    values[9] += lambda[1] * 2; // 3,3
  }

  return true;
}
\end{verbatim}

\paragraph{virtual void finalize\_solution(SolverReturn status,\\
    Index n, const Number* x, const Number* z\_L, const Number* z\_U, \\
    Index m, const Number* g, const Number* lambda, Number obj\_value)}
$\;$ \\
This is the only method that is not mentioned in Figure
\ref{fig.required_info}. This method is called by Ipopt after the
algorithm has finished (successfully or even with most errors).
\begin{itemize}
\item {\tt status}: (in), gives the status of the algorithm 
        as specified in {\tt IpAlgTypes.hpp},
        \begin{itemize}
        \item {\tt SUCCESS}: Algorithm terminated successfully 
        at a locally optimal point.
        \item {\tt MAXITER\_EXCEEDED}: Maximum number of iterations 
        exceeded (can be specified by an option).
        \item {\tt STOP\_AT\_TINY\_STEP}: Algorithm proceeds with very little
        progress. 
        \item {\tt STOP\_AT\_ACCEPTABLE\_POINT}: Algorithm stopped at a point
        that was converged, not to ``desired'' tolerances, 
        but to ``acceptable'' tolerances (see the ??? options).
        \item {\tt LOCAL\_INFEASIBILITY}: Algorithm converged to a point 
        of local infeasibility. Problem may be infeasible.
        \item {\tt RESTORATION\_FAILURE}: Restoration phase was called, but 
        failed to find a more feasible point.
        \item {\tt INTERNAL\_ERROR}: An unknown internal error occurred. Please
        contact the Ipopt Authors through the mailing list.
        \end{itemize}
\item {\tt n}: (in), the number of variables in the problem (dimension of {\tt x}). 
\item {\tt x}: (in), the current values for the primal variables, {\tt x}.
\item {\tt z\_L}: (in), the current values for the lower bound multipliers.
\item {\tt z\_U}: (in), the current values for the upper bound multipliers.
\item {\tt m}: (in), the number of constraints in the problem (dimension of {\tt g}).
\item {\tt g}: (in), the current value of the constraint residuals.
\item {\tt lambda}: (in), the current values of the equality multipliers.
\item {\tt obj\_value}: (in), the current value of the objective.
\end{itemize}

This method gives you the return status of the algorithm
(SolverReturn), and the values of the variables, 
the objective and constraint residuals when the algorithm exited.

In our example, we will print the values of some of the variables to 
the screen.

\begin{verbatim}
void HS071_NLP::finalize_solution(SolverReturn status,
                              Index n, const Number* x, const Number* z_L, const Number* z_U,
                              Index m, const Number* g, const Number* lambda,
                              Number obj_value)
{
  // here is where we would store the solution to variables, or write to a file, etc
  // so we could use the solution. 

  // For this example, we write the solution to the console
  printf("\n\nSolution of the primal variables, x\n");
  for (Index i=0; i<n; i++) {
    printf("x[%d] = %e\n", i, x[i]); 
  }

  printf("\n\nSolution of the bound multipliers, z_L and z_U\n");
  for (Index i=0; i<n; i++) {
    printf("z_L[%d] = %e\n", i, z_L[i]); 
  }
  for (Index i=0; i<n; i++) {
    printf("z_U[%d] = %e\n", i, z_U[i]); 
  }

  printf("\n\nObjective value\n");
  printf("f(x*) = %e\n", obj_value); 
}
\end{verbatim}

This is all that is required for our {\tt HS071\_NLP} class and 
the coding of the problem representation.
 
\subsubsection{Coding the Executable (main)}
Now that we have a problem representation, the {\tt HS071\_NLP} class,
we need to code the main function that will call Ipopt and ask Ipopt
to find a solution.

Here, we must create an instance of our problem ({\tt HS071\_NLP}), create an
instance of the ipopt solver (IpoptApplication), and ask the solver to
find a solution. We always use the SmartPtr template class instead of
raw C++ pointers when creating and passing Ipopt objects. To find out
more information about smart pointers and the SmartPtr implementation
used in Ipopt, see Appendix \ref{app.smart_ptr}.

Create the file {\tt MyExample.cpp} in the MyExample directory.
Include {\tt HS071\_NLP.hpp} and {\tt IpIpoptApplication.hpp}, tell
the compiler to use the Ipopt namespace, and implement the {\tt main}
function.
\begin{verbatim}
#include "IpIpoptApplication.hpp"
#include "HS071_NLP.hpp"

using namespace Ipopt;

int main()
{
        // use a SmartPtr to point the new HS071_NLP
        SmartPtr<TNLP> mynlp = new HS071_NLP();
        
        // use a SmartPtr to point to new IpoptApplication
        SmartPtr<IpoptApplication> app = new IpoptApplication();

        // Ask Ipopt to solve the problem
        SolverReturn status = app->OptimizeTNLP(mynlp);
        if (status == SUCCESSFUL) {
                std::cout << "SOLVED :)" << std::endl;
                return 0;
        }
        else {
                std::cout << "FAILED! :(" << std::endl;
                return -1;
        }
        
        // As the SmartPtr's go out of scope, the reference counts will be decremented
        // and the mynlp and app objects will automatically be deleted.
}
\end{verbatim} 

The first line of code in {\tt main} creates an instance of {\tt
HS071\_NLP}. We then create an instance of the ipopt solver,
IpoptApplication. The call to {\tt app->OptimizeTNLP(...)} will run
Ipopt and try to solve the problem. By default, Ipopt will write to
its progress to the console, and return the {\tt SolverReturn} status.

\subsubsection{Compiling and Testing the Example}
Our next task is to compile and test the code. If you are familiar
with the compiler and linker used on your system, you can build the
code, including the ipopt library (and other necessary libraries).  If
you are using Linux/UNIX, then a sample makefile exists already that was
created by configure. Copy {\tt Examples/hs071\_cpp/Makefile} into
your {\tt MyExample} directory. This makefile was created for the
hs071\_cpp code, but it can be easily modified for your example
problem. Edit the file, making the following changes,

\begin{itemize}
\item change the {\tt EXE} variable \\
{\tt EXE = my\_example}
\item change the {\tt OBJS} variable \\
{\tt OBJS = HS071\_NLP.o MyExample.o}
\end{itemize}
and the problem should compile easily with, \\
{\tt \$ make}
Now run the executable,\\ 
{\tt \$ ./my\_example}
and you should see output resembling the following,
\begin{verbatim}
Total number of variables............................:        4
                     variables with only lower bounds:        0
                variables with lower and upper bounds:        4
                     variables with only upper bounds:        0
Total number of equality constraints.................:        1
Total number of inequality constraints...............:        1
        inequality constraints with only lower bounds:        1
   inequality constraints with lower and upper bounds:        0
        inequality constraints with only upper bounds:        0
 
 iter     objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls
    0   1.7159878e+01 2.01e-02 5.20e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0 y
    1   1.7146308e+01 1.63e-01 1.47e-01  -1.0 1.15e-01    -  9.86e-01 1.00e+00f  1
    2   1.7065508e+01 3.10e-02 8.47e-02  -1.7 1.99e-01    -  9.54e-01 1.00e+00h  1 Nhj
    3   1.7002626e+01 4.10e-02 4.81e-03  -2.5 5.52e-02    -  1.00e+00 1.00e+00h  1
    4   1.7019082e+01 1.20e-03 1.81e-04  -2.5 1.10e-02    -  1.00e+00 1.00e+00h  1
    5   1.7014253e+01 1.80e-04 4.87e-05  -3.8 4.86e-03    -  1.00e+00 1.00e+00h  1
    6   1.7014020e+01 9.25e-07 2.15e-07  -5.7 2.76e-04    -  1.00e+00 1.00e+00h  1
    7   1.7014017e+01 1.01e-10 2.60e-11  -8.6 3.32e-06    -  1.00e+00 1.00e+00h  1
 
Number of Iterations....: 7
 
                                   (scaled)                 (unscaled)
Objective...............:   1.7014017145177885e+01    1.7014017145177885e+01
Dual infeasibility......:   2.5980210027546616e-11    2.5980210027546616e-11
Constraint violation....:   1.8175683180743363e-11    1.8175683180743363e-11
Complementarity.........:   2.5282956951655172e-09    2.5282956951655172e-09
Overall NLP error.......:   2.5282956951655172e-09    2.5282956951655172e-09
 
 
Number of objective function evaluations             = 8
Number of objective gradient evaluations             = 8
Number of equality constraint evaluations            = 8
Number of inequality constraint evaluations          = 8
Number of equality constraint Jacobian evaluations   = 8
Number of inequality constraint Jacobian evaluations = 8
Number of Lagrangian Hessian evaluations             = 9
 
EXIT: Optimal Solution Found.
 
 
Solution of the primal variables, x
x[0] = 1
x[1] = 4.743
x[2] = 3.82115
x[3] = 1.37941
 
 
Solution of the bound multipliers, z_L and z_U
z_L[0] = 1.08787
z_L[1] = 6.69317e-10
z_L[2] = 8.8877e-10
z_L[3] = 6.57011e-09
z_U[0] = 6.26262e-10
z_U[1] = 9.78906e-09
z_U[2] = 2.12283e-09
z_U[3] = 6.92528e-10
 
 
Objective value
f(x*) = 17.014
 
 
*** The problem solved!
\end{verbatim}

This completes the basic C++ tutorial, but see Section
\ref{sec.output} which explains the standard console output of Ipopt,
Section \ref{sec.jnlst} to learn about adjusting the output produced
from Ipopt through the use of the Journalist, and Section
\ref{sec.options} for information about the use of options to
customize the behavior of Ipopt.

\subsection{The C Interface}
Have a look at {\tt IpStdCInterface.h} to see the declarations for the
C interface.  For the C++ interface, all problem information is
provided through the NLP class. In the C interface, we instead create
an IpoptProblem (C structure) and pass that structure to the
IpoptSolve call.

The IpoptProblem structure contains the problem dimensions, the variable
and constraint bounds, and the function pointers for callbacks that 
will be used to evaluate the NLP.
We then make the call to IpoptSolve, giving Ipopt the IpoptProblem structure, 
the starting point, and arrays to store the solution values, if desired.

A completed version of this example can be found in {\tt
Examples/hs071\_c}.  We first create the necessary callback functions
for evaluating the NLP.  We require callbacks to evaluate the
objective value, constraints, gradient of the objective, Jacobian of
the constraints, and the Hessian of the Lagrangian.  These callbacks
are implemented using function pointers (see {\tt IpStdCInterface.h}).
Have a look at the documentation for {\tt eval\_f}, {\tt eval\_g}, {\tt
eval\_grad\_f}, {\tt eval\_jac\_g}, and {\tt eval\_h} in Section
\ref{sec.cpp_problem}. The C implementations will have different
prototypes, but will be implemented almost identically to the C++
code.

Create a new directory {\tt MyCExample} and create a new file, {\tt
hs071\_c.c}.  Here, include the interface header file {\tt
IpStdCInterface.h}, along with {\tt malloc.h} and {\tt assert.h}. Add
the prototypes and implementations for the five callback functions.
See the completed example in {\tt Examples/hs071\_c/hs071\_c.c}.

We now need to implement the {\tt main} function, create the {\tt
IpoptProblem}, and call {\tt IpoptSolve}. The {\tt CreateIpoptProblem}
function requires the problem dimensions, the variable and constraint
bounds, and the function pointers to the callback routines. The {\tt
IpoptSolve} function requires the {\tt IpoptProblem}, the starting
point, and allocated arrays for the solution.  The {\tt main} function
from the example is shown below.
\begin{verbatim}
int main()
{
  Index n=-1;                          /* number of variables */
  Index m=-1;                          /* number of constraints */
  Number* x_L = NULL;                  /* lower bounds on x */
  Number* x_U = NULL;                  /* upper bounds on x */
  Number* g_L = NULL;                  /* lower bounds on g */
  Number* g_U = NULL;                  /* upper bounds on g */
  IpoptProblem nlp = NULL;             /* IpoptProblem */
  enum ApplicationReturnStatus status; /* Solve return code */
  Number* x = NULL;                    /* starting point and solution vector */
  Number obj;                          /* objective value */
  Index i;                             /* generic counter */
  
  n=4;
  x_L = (Number*)malloc(sizeof(Number)*n);
  x_U = (Number*)malloc(sizeof(Number)*n);
  for (i=0; i<n; i++) {
    x_L[i] = 1.0;
    x_U[i] = 5.0;
  }

  m=2;
  g_L = (Number*)malloc(sizeof(Number)*m);
  g_U = (Number*)malloc(sizeof(Number)*m);
  g_L[0] = 25; g_U[0] = 2e19;
  g_L[1] = 40; g_U[1] = 40;

  nlp = CreateIpoptProblem(n, x_L, x_U, m, g_L, g_U, 8, 10, 0, 
			   &eval_f, &eval_g, &eval_grad_f, 
			   &eval_jac_g, &eval_h);
  
  
  x = (Number*)malloc(sizeof(Number)*n);
  x[0] = 1.0;
  x[1] = 5.0;
  x[2] = 5.0;
  x[3] = 1.0;

  AddIpoptNumOption(nlp, "tol", 1e-9);
  AddIpoptStrOption(nlp, "mu_strategy", "adaptive");

  status = IpoptSolve(nlp, x, NULL, &obj, NULL, NULL, NULL, NULL);
 
  FreeIpoptProblem(nlp);
  free(x_L);
  free(x_U);
  free(g_L);
  free(g_U);
  free(x);

  return 0;
}
\end{verbatim}
Here, we declare all the necessary variables and set the dimensions of
the problem.  The problem has 4 variables, so we set {\tt n} and
allocate space for the variable bounds (don't forget to call {\tt
free} for each of your {\tt malloc} calls). We then set the values for
the variable bounds.  

The problem has 2 constraints, so we set {\tt m}
and allocate space for the constraint bounds. The first constraint has
a lower bound of $25$ and no upper bound.  Here we set the upper bound
to 2e19. Ipopt interprets any number greater than nlp_upper_bound_inf
as infinity. The default value of nlp_upper_bound_inf and
nlp_lower_bound_inf is 1e19 and can be changed through ipopt options.
The second constraint is an equality, so we set both the upper and the 
lower bound to $40$.

We next create an instance of the {\tt IpoptProblem} by calling {\tt
CreateIpoptProblem}, giving it the problem dimensions and the variable
and constraint bounds. The arguments {\tt nele\_jac} and {\tt
nele\_hess} are the number of elements in Jacobian and the Hessian
respectively. See appendix \ref{app.triplet} for a description of the
sparse matrix format. The {\tt index_style} argument specifies whether
we want to use C style indexing for the row and column indices of the
matrices or Fortran style indexing. Here, we set it to {\tt 0} to
indicate C style.  We also include the references to each of our
callback functions. Ipopt can use these function pointers to ask for
evaluation of the NLP when required.

The next two lines illustrate how you can change the value of options
through the interface.  Ipopt Options can also be changed by creating
a PARAMS.DAT file. We next allocate space for the initial point and
set the values as given in the problem definition.

The call to {\tt IpoptSolve} can provide us with information about the solution,
but most of this is optional. Here, we want values for the bound multipliers at
the solution and we allocate space for these.

We can now make the call to {\tt IpoptSolve} and find the solution of
the problem. We pass in the {\tt IpoptProblem}, the starting point
{\tt x} (Ipopt will use this variable to return the solution as
well). The next 5 arguments are pointers so Ipopt can fill in values
at the solution.  If these pointers are set to {\tt NULL}, Ipopt will
ignore that entry.  For example, here, we do not want the constraint
residuals at the solution or the equality multipliers, so we set those
entries to {\tt NULL}. We do want the value of the objective, and the
multipliers for the variable bounds. The last argument is a {\tt
void*} for user data. Any pointer you give here will also be passed to
you in the callback functions.

The return code is an ApplicationReturnStatus enumeration, see {\tt
Interfaces/ReturnCodes_inc.h}.

After the problem has solved, we check the status and print the solution if 
successful. Finally, we free the memory and return from {\tt main}.

\section*{Acknowledgement}
The initial version of this document was created by Yoshiaki Kawajir
as a course project for \textit{47852 Open Source Software for
Optimization}, taught by Prof. Fran\c cois Margot at Tepper School of
Business, Carnegie Mellon University. The authors of Ipopt greatly
thank Yoshi for his efforts.

\begin{thebibliography}{99}
\bibitem{COINORWeb}
http://www.coin-or.org
\bibitem{AndreasPaper}
W\"achter, A. and Biegler, L.T.:''On the Implementation of a Primal-Dual
        Interior Point Filter Line Search Algorithm for Large-Scale
        Nonlinear Programming'', Research Report, IBM T. J. Watson
        Research Center, Yorktown, USA (2004)
\bibitem{AndreasThesis}
W\"achter, A.:''An Interior Point Algorithm for Large-Scale Nonlinear
        Optimization with Applications in Process Engineering'',
        Ph.D. Thesis, Carnegie Mellon University, Pittsburgh, USA (2002)
\bibitem{MargotClassText}
Margot, F.: Course material for \textit{47852 Open Source Software for
        Optimization}, Carnegie Mellon University (2005)
\end{thebibliography}


\appendix
\section{Triplet Format for Sparse Matrices}\label{app.triplet}
Ipopt was designed for optimizing large sparse nonlinear programs. 
Because of problem sparsity, the required matrices (like the Jacobian or Hessian) are not stored as traditional dense matrices, but rather in a sparse matrix format. For the tutorials in this document, we use the triplet format. 
Consider the matrix,
\[
\left[
\begin{array}{ccccccc}
1.1     & 0             & 0             & 0             & 0             & 0             & 0.5 \\
0       & 1.9   & 0             & 0             & 0             & 0             & 0.5 \\
0       & 0             & 2.6   & 0             & 0             & 0             & 0.5 \\
0       & 0             & 7.8   & 0.6   & 0             & 0             & 0    \\
0       & 0             & 0             & 1.5   & 2.7   & 0             & 0     \\
1.6     & 0             & 0             & 0             & 0.4   & 0             & 0     \\
0       & 0             & 0             & 0             & 0             & 0.9   & 1.7 \\
\end{array}
\right].
\]

A standard dense matrix representation would need to store $7 \cdot
7{=} 49$ floating point numbers, where many entries would be zero. In
triplet format, however, only the nonzero entries are stored. The
triplet format records the row number, the column number, and the
value of all nonzero entries in the matrix. For the matrix above, this
means storing $14$ integers for the rows, $14$ integers for the
columns, and $14$ floating point numbers for the values. While this
does not seem like a huge space savings over the $49$ floating point
numbers stored in the dense representation, for larger matrices, the
space savings are very dramatic\footnote{For an $n \times n$ matrix,
  the dense representation grows with the the square of $n$, while the
  sparse representation grows linearly in the number of nonzeros.}

In triplet format used in these Ipopt interfaces, the row and column
numbers are 1-based {\bf have table for both 0- and 1-based}, and the
above matrix is represented by
\[
\begin{array}{ccc}
row     &       col     &       value \\
1       &       1       &       1.1     \\
1       &       7       &       0.5     \\
2       &       2       &       1.9     \\
2       &       7       &       0.5     \\
3       &       3       &       2.6     \\
3       &       7       &       0.5     \\
4       &       3       &       7.8     \\
4       &       4       &       0.6     \\
5       &       4       &       1.5     \\
5       &       5       &       2.7     \\
6       &       1       &       1.6     \\
6       &       5       &       0.4     \\
7       &       6       &       0.9     \\
7       &       7       &       1.7
\end{array}
\]

The individual elements of the matrix can be listed in any order, and
if there are multiple items for the same nonzero position, the values
provided for those positions are added.

MISSING: Symmetric matrices


\section{The Smart Pointer Implementation: SmartPtr}

The SmartPtr class is described in {\tt IpSmartPtr.hpp}. It is a
template class that takes care of deleting objects for us so we need
not be concerned about memory leaks. Instead of pointing to an object
with a raw C++ pointer (e.g. {\tt HS071\_NLP*}), we use a SmartPtr.
Every time a SmartPtr is set to reference an object, it increments a
counter in that object (see the ReferencedObject base class if you are
interested). If a SmartPtr is done with the object, either by leaving
scope or being set to point to another object, the counter is
decremented. When the count of the object goes to zero, the object is
automatically deleted. SmartPtr's are very simple, just use them as
you would a standard pointer.

It is very important to use SmartPtr's instead of raw pointers when
passing objects to Ipopt. Internally, Ipopt uses smart pointers for
referencing objects. If you use a raw pointer in your executable, the
object's counter will NOT get incremented. Then, when Ipopt uses smart
pointers inside its own code, the counter will get
incremented. However, before Ipopt returns control to your code, it
will decrement as many times as it incremented and the counter will
return to zero. Therefore, Ipopt will delete the object. When control
returns to you, you now have a raw pointer that points to a deleted
object.

This might sound difficult to anyone not familiar with the use of
smart pointers, but just follow one simple rule; always use a SmartPtr
when creating or passing an Ipopt object.

\end{document}
