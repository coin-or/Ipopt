<html>

<STYLE type="text/css">
code, pre { font:        12px monospace;
            color:       #000066 }

p, ul { font:  12px Geneva, Arial, Helvetica, sans-serif;
    color: #000000 ;}

h2 { font:  20px Geneva, Arial, Helvetica, sans-serif;
     font-weight: bold;
     color: #000000; }

h3 { font:  16px Geneva, Arial, Helvetica, sans-serif;
     font-weight: bold;
     color: #000000; }
</STYLE> 

<title>MATLAB interface for IPOPT: installation and usage</title>
<body bgcolor="#FFFFFF">

<center>
<br>
<table align="center" border=0 width=460 cellspacing=0
cellpadding=0>
<tr><td valign=top>

<center><h2>How to install and use the MATLAB interface for IPOPT</h2>
</center>

<p><a href="https://projects.coin-or.org/Ipopt">IPOPT</a> is a
software package for solving nonlinear objectives subject to nonlinear
constraints. It uses primal-dual interior point
methodology. Importantly, it is open source. What is described here is an
interface to the IPOPT optimization engine, specially designed so it can be 
easily used in the <a href="www.mathworks.com/products/matlab/">MATLAB</a> 
programming environment.</p> 

<p>Let's start by installing the MATLAB interface.</p>

<a name="install"><h3>Installation</h3>

<p>Throughout this tutorial, we'll be assuming you have some sort of
UNIX-based operating system, such as Mac OS X, Solaris or Linux. We
can't help you if you have a Windows operating system, but we'll
presume you have enough experience with Windows to figure out how to
modify these steps for your setup, if necessary. Of course, you will
need to have a version of MATLAB installed on your computer (hopefully
a more recent version), and that you are relatively familiar with the
MATLAB programming language (if not, it is a very simple language and
you could learn it in a few hours). This software has been tested on
MATLAB versions 7.2 and 7.3. It might very well work on earlier
versions of MATLAB, but there is also a good chance that it will
not. It is unlikely that the software will run with versions prior to
MATLAB 6.5.</p>

<p><b>Install compilers.</b> The first thing you need to do is
install a C++ compiler and a Fortran 77 compiler. Now, you might
already have these installed. However, you may not be able to use
these installed compilers because you must use the precise compilers
supported by MATLAB (a pain, I know). For instance, on my Linux
machine I have MATLAB version 7.3, so the people at MathWorks tell me
that I need to have the <a
href="href=http://www.gnu.org/software/gcc">GNU Compiler
Collection</a> (GCC) version 3.4.5. If you use the incorrect version
of GCC, you will likely encounter linking errors (and the "mex"
command will tell you which compiler versions are Ok). In order to
find out which compiler is supported by your version of MATLAB, run
the <code>mex</code> program provided in your MATLAB installation or
consult <a
href="http://www.mathworks.com/support/tech-notes/1600/1601.html">this
webpage</a>.</p>

<p><b>Configure MATLAB.</b> Once you've installed the appropriate
compilers, set up and configure MATLAB to build MEX files. This is
explained quite nicely <a
href="http://www.mathworks.com/support/tech-notes/1600/1605.html">here</a>.</p>
<p><b>Install IPOPT.</b> The MATLAB interface adds several complicating
matters to the <a
href="http://www.coin-or.org/Ipopt/documentation">standard IPOPT
installation procedure</a>. Please familiarize yourself with the
standard procedure. What follows are the steps followed on a typical
Linux machine. First, download the Ipopt source files and the
third-party source code (BLAS, LAPACK, HSL, etc.).</p>

<p>MATLAB demands that you compile the code with the certain flags, such
as <code>-fPIC</code> and <code>-fexceptions</code> on Linux. The
first flag tells the compiler to generate position-independent code,
and the second flag enables exception handling. Usually these flags
coincide with your MEX options file. You can see those options by
running the mex compiler with the <code>-v</code> flag on a simple
example source file (Hello World is your friend). See <a
href="http://www.mathworks.com/support/tech-notes/1600/1605.html">this</a>
MathWorks technical support webpage for more information on the
options file.</p>

<p>Once you have all the necessary source code, call the IPOPT configure 
script. On a Linux machine, the call should look something like</p>

<p><code><pre>  ./configure --prefix=$HOME/ipopt/install            \
       CXX=g++-3.4.5 CC=gcc-3.4.5 F77=g77-3.4.5       \
       ADD_CXXFLAGS="-fPIC -fexceptions"              \
       ADD_CFLAGS="-fPIC -fexceptions"                \
       ADD_FFLAGS="-fPIC -fexceptions"</pre></code></p>

<p>After this, follow the standard installation steps: type
<code>make</code> then <code>make install</code> in the UNIX command
line. This compiles all the source code into a single
library. However, it has not compiled the code for the MATLAB
interface. We'll do this next.</p>

<p><b>Modify the Makefile.</b> Go to into the directory
<code>Ipopt/contrib/MatlabInterface/src</code> and open the file
called <code>Makefile</code> with your favourite text editor. We need
to change this file a little bit so that it coincides with your MATLAB
setup. You will find that most of the variables, such as
<code>CXX</code> and <code>CXXFLAGS</code>, have been automatically
set according to the flags specified during the initial call to
<code>configure</code>. However, you may need to modify
<code>MATLAB_HOME</code> and <code>MEXSUFFIX</code>, as explained in
the comments of the Makefile.</p>

<p>There's a great possibility you will encounter problems with the
installation instructions we have just described here. I'm afraid some
resourcefulness will be required on your part, as the installation
will be slightly different for each person. Please consult the <a
href="#trouble">troubleshooting section</a> on this webpage, and the
<a href="http://list.coin-or.org/pipermail/coin-ipopt">archives</a>
of the IPOPT mailing list. If you can't find the answer at either of
these locations, try sending an email to the <a
href="http://list.coin-or.org/mailman/listinfo/coin-ipopt">IPOPT
mailing list</a>.</p>

<p><b>Finally.</b> If the installation procedure was successful, you
will end up with a MEX file. On a Linux machine, the MEX file will be
called <code>ipopt.mexglx</code>. In order to use it in MATLAB, you
need to tell MATLAB where to find it. The best way to do this is to
type</p>

<p><code><pre>  addpath sourcedir</pre></code></p>

<p>in the MATLAB command prompt, where <code>sourcedir</code> is the
location of the MEX file you created. You can also achieve the same
thing by modifying the <code>MATLABPATH</code> environment variable in
the UNIX command line, using either the <code>export</code> command
(in Bash shell), or the <code>setenv</code> command (in C-shell).</p>

<a name="tutorial"><h3>Tutorial by example</h3>

<p>Let's go through four examples which demonstrate the principal
features of the MATLAB interface to IPOPT. For additional information,
you can always type <code>help ipopt</code> in the MATLAB prompt. The
tutorial exampels are all located in the directory
<code>Ipopt/contrib/MatlabInterface/examples</code>.</p>

<p>First, let's look at the Hock & Schittkowski test problem #51.<a
href="#footnote1"><sup>1</sup></a> It is an optimization problem with
5 variables, no inequality constraints and 3 equality constraints. In
MATLAB, go to the subdirectory <code>hs051</code>. In this
subdirectory, there is a MATLAB script <code>examplehs051.m</code>
which runs the limited-memory BFGS (Broyden-Fletcher-Goldfarb-Shanno)
algorithm with the starting point <code>[2.5 0.5 2 -1 0.5]</code> and
obtains the solution <code>[1 1 1 1 1]</code>. The line in the script
which executes the IPOPT solver is</p>

<p><code><pre>   x = ipopt(x0,lb,ub,lbc,ubc,'computeObjectiveHS051',
             'computeGradientHS051','computeConstraintsHS051',...
 	     'computeJacobianHS051','',[],'',...
 	     'hessian_approximation','limited-memory',...
 	     'mu_strategy','adaptive','tol',1e-7);</pre></code></p>

<p>The first input is the initial point. The second and third inputs
specify the lower and upper bounds on the variables. Since there are
no such bounds, we set the entries of these two vectors to
<code>-Inf</code> and <code>+Inf</code>. The fourth and fifth inputs
specify the lower and upper bounds on the 3 constraint functions. The
next five inputs specify the required callback routines. Since we are
using a limited-memory approximation to the Hessian, we don't need to
know the values of the second-order partial derivatives, so we set the
Hessian callback routine to the empty string. The rest of the input
arguments set some options for the solver, as detailed in the IPOPT
documentation.</p>

<p>If you examine the files <code>computeObjectiveHS051.m</code> and
<code>computeGradientHS051.m</code> in the same directory, you will
see that computing the objective function and gradient vector is
relatively straightforward. The M-file
<code>computeConstraintsHS051.m</code> returns a vector of length
equal to the number of constraint functions. The callback function
<code>computeJacobianHS051.m</code> returns an M x N sparse matrix,
where M is the number of constraint functions and N is the number of
variables. It is important to always return a sparse matrix, even if
there is no computational advantage in doing so. Otherwise, MATLAB
will report an error.</p>

<p>Next, let's move to the second example, <code>examplehs038.m</code>
in the subdirectory <code>hs038</code>. It demonstrates the use of
IPOPT on an optimization problem with 4 variables and no constraints
other than simple bound constraints. This time, we've implemented a
callback routine for evaluating the Hessian. The Hessian callback
function takes as input the current value of the variables
<code>x</code>, the factor in front of the objective term
<code>sigma</code>, an the values of the constraint multipliers
<code>lambda</code> (which in this case is empty). If the last input
is true, then the callback routine must return a sparse matrix that
has zeros in locations where the second-order derivative at that
location will ALWAYS be zero. The return value H must always be a
lower triangular matrix (type <code>help tril</code>). As explained in
the IPOPT documentation, the Hessian matrix is symmetric so the
information contained in the upper triangular part is redundant.</p>

<p>This example also demonstrates the use an iterative callback
function, which can be useful for displaying the status of the
solver. This is specified by the twelfth input. The M-file
<code>genericcallback.m</code> takes as input the current iteration
<code>t</code>, the current value of the objective <code>f</code>, and
the current point <code>x</code>.</p>

<p>The third slightly more complicated example script is
<code>examplehs071.m</code>, which is the same as the problem explored
in the IPOPT documentation (Hock and Schittkowski test problem
#71). It is worth taking a peek at the files
<code>computeHessianHS071.m</code> and
<code>computeJacobianHS071.m</code>. In the Hessian callback function,
we make use of the input lambda. Since the Hessian is dense, its
structure is returned with the line</p>

<p><code><pre>   H = sparse(tril(ones(n)))</pre></code></p>

<p>where <code>n</code> is the number of variables. The Jacobian is
dense, so we can return its structure in a single line:</p>

<p><code><pre>   J = sparse(ones(m,n))</pre></code></p>

<p>where <code>m</code> is the number of constraint functions.</p>

<p>The last example is the script <code>examplelauritzen.m</code>. It
is vastly more complicated than the other three. It pertains to
research in inference in probabilistic models in artificial
intelligence. This script demonstrates the problem of inferring the
most probable states of random variables given a Bayesian network.<a
href="#footnote2"><sup>2</sup></a> In this case, the model represents
the interaction between causes (e.g. smoking) and diseases (e.g. lung
cancer). We are given a patient that is a smoker, has tested positive
for some X-ray, and has recently visited Asia. In many ways this is a
silly and highly overused example, but it suits our needs here because
it demonstrates how to treat inference as an optimization problem, and
how to solve this optimization problem using IPOPT. This code should
NOT be used to solve large inference problems because it is not
particularly efficient.</p>

<p>The call to IPOPT is buried in the file <code>bopt.m</code>. It is</p>

<p><code><pre>[qR qS] = ipopt({qR qS},{ repmat(eps,nqr,1) repmat(eps,nqs,1) },...
                { repmat(inf,nqr,1) repmat(inf,nqs,1) },...
                [ ones(1,nr) ones(1,ns) zeros(1,nc) ],...
                [ ones(1,nr) ones(1,ns) zeros(1,nc) ],...
                'computeJGObjective','computeJGGradient',...
                'computeJGConstraints','computeJGJacobian',...
                'computeJGHessian',{ K C f Rv Rf Sv Sf NS d },'',...
                'mu_strategy','adaptive','max_iter',maxiter,...
                'tol',tolerance,'print_level',verbose*4);</pre></code></p>

<p>As you can see, it is rather complicated! The first input, as
usual, is the starting point. (The variables actually represent
probability estimates.) Notice that we are passing a cell array, and
each entry of the cell array is a matrix. Likewise, the bound
constraints are specified as cell arrays. This is permitted as long as
the starting point and the bound constraints have the same
structure. If not, the MATLAB will report an error. (Note that the
lower bounds on the variables are set to floating-point
precision. Type <code>help eps</code>. In this way, we ensure that the
logarithm of a variable never evaluates to zero.)</p>

<p>This cell array syntax is useful when your program has several
different types of variables. These sets of variables are then passed
as separate input arguments to the MATLAB callback functions. For
instance, <code>qR</code> and <code>qS</code> are passed as separate
arguments to the objective callback function
<code>computeJGObjective.m</code>.  The entries of the cell array are
also treated as separate outputs from the gradient callback function
(see <code>computeJGGradient.m</code>), and from the main call to
ipopt.</p>

<p>In this example, the constraint functions are all linear, so they have
no impact on the value of the Hessian. In fact, the Hessian is a
diagonal matrix (but not positive definite). The Jacobian can be
extremely large, but it is also very sparse; the number of entries is
a multiple of the number of variables.</p>

<p>This example also demonstrates the use of auxiliary data. In
<code>bopt.m</code>, notice that the input after
<code>computeJGHessian</code> is a cell array. This cell array is
passed as input to every MATLAB callback routine.</p>

<p>The tutorial is over!</p>

<a name="notes"><h3>Notes on implementation of the MATLAB interface</h3>

<p>We won't bore you with all the details of the implementation. We
would, however, like to briefly point out a few of them. Most of the
issues of interest surround the representation of sparse matrices.</p>

<p>The MATLAB interface will necessarily be slower than the standard
C++ interface to IPOPT. That's because MATLAB dynamically allocates
new memory for all the outputs passed back from a function. Thus, for
large problems each iteration of IPOPT will involve the dynamic
allocation and deallocation of large amounts of memory.</p>

<p><b>Sparse matrices.</b> The greatest challenge was most definitely
the conversion of sparse matrices from MATLAB to IPOPT. Sparse
matrices are used to represent the Jacobian of the constraint
functions and the Hessian of the Lagrangian function. There is a very
nice <a href="http://link.aip.org/link/?SML/13/333">document</a> by
Gilbert, Moler and Schreiber that discusses the design and
implementation of sparse matrices in the MATLAB environment. The
problem is that IPOPT assumes a static sparse matrix structure, but in
MATLAB there is no way to ensure that the size of the matrix (the
number of non-zero elements) does not change over time; if an entry of
a sparse matrix is set to zero, then the arrays are automatically
adjusted so that no storage is expended for that entry. This may seem
like a highly inefficient way to implement sparse matrices, and indeed
it is. However, Gilbert, Moler and Schreiber emphasize efficient
matrix-level operations over efficient element-level operations.</p>

<p>We can legitimately make the following assumption: the non-zero
entries of a sparse matrix passed back from MATLAB are a
<emph>subset</emph> of the non-zero entries in IPOPT's respective
sparse matrix.</p>

<p>The class <code>SparseMatrixStructure</code> keeps track of the
structure of a sparse MATLAB matrix. It does not store the values of
the non-zero entries. We use it for the Jacobian of the constraints
and the Hessian of the Lagrangian. Even though these two matrices are
fundamentally different (one is square and lower triangular, the other
is rectangular), we can treat their structures in the same way.</p>

<p>The principal functions of interest in class
<code>SparseMatrixStructure</code> are <code>getColsAndRows</code> and
<code>copyElems</code>. The function <code>getColsAndRows</code>
converts the MATLAB sparse matrix format into the equivalent IPOPT
format. The function <code>copyElems</code> copies the entries from
one sparse matrix to another when one sparse matrix has a different
structure than the other. Obviously, for the copy operation to be
plausible, the set of non-zero entries of the destination matrix must
be a superset of the non-zero entries of the source matrix. Due to
efficiency concerns, no checks are made to ensure that this is
satisfied; it is up to the user to ensure that a sparse matrix passed
back to IPOPT never has a non-zero entry that was not declared
initially when <code>returnStructureOnly = true</code>. In my
implementation, I have gone through great pains to ensure: 1. that the
copy function only makes one pass through the entries of the sparse
matrix, and 2. that there are no if statements inside the for loops,
which can severely impinge on the speed of the copy operation.</p>

<a name="trouble"><h3>Troubleshooting and known issues</h3>

<p>The installation procedure we have described does involve a certain
amount of expertise on the part of the user. If you are encountering
problems, it is highly recommended that you follow the standard
installation of IPOPT first, and then test the installation by running
some examples, either in C++ or in AMPL.</p>

<p>Even if you didn't get any errors during compilation, there's still
a possibility that you didn't link the MEX file properly, in which
case executing IPOPT in MATLAB will cause MATLAB to crash. This is a
common problem, and usually arises because you did not choose the
proper compiler or set the proper compilation flags when you ran the
<code>configure</code> script at the very beginning.</p>

<h3>Footnotes</h3>

<p><a name="footnote1"><sup>1</sup></a> For
further information, see: Willi Hock and Klaus Schittkowski. (1981)
Test Examples for Nonlinear Programming Codes. Lecture Notes in
Economics and Mathematical Systems Vol. 187, Springer-Verlag.</p>

<p><a name="footnote2"><sup>2</sup></a> If you want to learn more
     about junction graphs for approximate inference, and a special
     case of this is called the <emph>junction tree
     algorithm</emph>. Recommended reading includes: <ul><li>Aji and
     McEliece (2001). The generalized distributive law and free energy
     minimization. Proceedings of the 39th Allerton Conference.  <li>
     Yedidia, Freeman and Weiss (2005). Constructing free-energy
     approximations and generalized belief propagation algorithms.
     IEEE Transactions on Information Theory 51(7).</ul></p>

</html>
