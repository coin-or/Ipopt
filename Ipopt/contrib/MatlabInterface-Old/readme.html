<html>

<STYLE type="text/css">
code { font:        12px monospace;
       color:       #000066 }

pre { font:        10px monospace;
      color:       #000066 }

p, ul, h2, h3, h4 { 
  font: 12px Geneva, Arial, Helvetica, sans-serif;
  color:#000000;
}

h2, h3, h4 { font-weight:bold; }

h2 { font-size:20px; }
h3 { font-size:16px; }
h4 { font-size:14px; 
     margin-bottom:0px; }

</STYLE> 

<title>MATLAB interface for IPOPT: installation and usage</title>
<body bgcolor="#FFFFFF">

<center>
<br>
<table align="center" border=0 width=460 cellspacing=0
cellpadding=0>
<tr><td valign=top>

<center><h2>How to install and use the MATLAB interface for IPOPT</h2>
<p>Peter Carbonetto<br>
Department of Computer Science<br>
University of British Columbia</p>
</center>

<p><a href="https://projects.coin-or.org/Ipopt">IPOPT</a> is a
software package for solving nonlinear objectives subject to nonlinear
constraints. It uses primal-dual interior point
methodology. Importantly, it is open source. What is described here is an
interface to the IPOPT optimization engine, specially designed so it can be 
easily used in the <a href="www.mathworks.com/products/matlab/">MATLAB</a> 
programming environment.</p> 

<p>Let's start this tutorial by installing the MATLAB interface.</p>

<a name="install"><h3>Installation</h3>

<p>Throughout this tutorial, we'll be assuming you have some sort of
UNIX-based operating system, such as Mac OS X, Solaris or Linux. We
can't help you if you have a Windows operating system, but we'll
presume you have enough experience with Windows to figure out how to
modify these steps for your setup, if necessary. (If you are using 
Windows, we highly recommend that you check out the 
<a href="http://gnumex.sourceforge.net">gnumex</a> project website before
continuing.) Of course, you'll need to have a version of MATLAB installed o
n your computer (hopefully
a more recent version), and that you are relatively familiar with the
MATLAB programming language (if not, it is a very simple language and
you could learn it in a few hours). This software package has been
tested on MATLAB versions 7.2 and 7.3. It might very well work on
earlier versions of MATLAB, but there is also a good chance that it
will not. It is unlikely that the software will run with versions
prior to MATLAB 6.5.</p>

<p><b>Install compilers.</b> The first thing you need to do is install
a C++ compiler and a Fortran 77 compiler. Now, you might already have
these installed. However, you may not be able to use these installed
compilers because you must use the precise compilers supported by
MATLAB (yes, it's a pain). For instance, on my Linux machine I have
MATLAB version 7.3, so the people at MathWorks tell me that I need to
have the <a href="href=http://www.gnu.org/software/gcc">GNU Compiler
Collection</a> (GCC) version 3.4.5. If you use the incorrect version
of GCC, you will likely encounter linking errors (and the "mex"
command will tell you which compiler versions are Ok). In order to
find out which compiler is supported by your version of MATLAB, run
the <code>mex</code> program provided in your MATLAB installation or
consult <a
href="http://www.mathworks.com/support/tech-notes/1600/1601.html">this
webpage</a>.</p>

<p><b>Configure MATLAB.</b> Once you've installed the appropriate
compilers, set up and configure MATLAB to build MEX files. This is
explained quite nicely <a
href="http://www.mathworks.com/support/tech-notes/1600/1605.html">here</a>.</p>
<p><b>Install IPOPT.</b> The MATLAB interface adds several
complicating matters to the <a
href="http://www.coin-or.org/Ipopt/documentation">standard IPOPT
installation procedure</a>. Before you continue, please familiarize
yourself with the standard procedure. What follows are the steps
followed on a typical Linux machine. First, download the IPOPT source
files and the third-party source code (BLAS, LAPACK, HSL, etc.).</p>

<p>MATLAB demands that you compile the code with certain flags, such
as <code>-fPIC</code> and <code>-fexceptions</code> (on Linux). The
first flag tells the compiler to generate position-independent code,
and the second flag enables exception handling. Usually these flags
coincide with your MEX options file. You figure out which flags are
used on your system by running the mex compiler with the
<code>-v</code> flag on a simple example source file (<a href="http://www.mathworks.com/support/tech-notes/1600/1605.html#example1">Hello
World</a> is your friend). See <a
href="http://www.mathworks.com/support/tech-notes/1600/1605.html">this</a>
MathWorks technical support webpage for more information on the
MEX options file.</p>

<p>Once you have all the necessary source code, call the IPOPT
configure script. On a Linux machine with MATLAB 7.3 installed, the
call should look something like</p>

<p><code><pre>  ./configure --prefix=$HOME/ipopt/install      \
       CXX=g++-3.4.5 CC=gcc-3.4.5 F77=g77-3.4.5 \
       ADD_CXXFLAGS="-fPIC -fexceptions"        \
       ADD_CFLAGS="-fPIC -fexceptions"          \
       ADD_FFLAGS="-fPIC -fexceptions"</pre></code></p>

<p>We also installed the MATLAB interface to IPOPT on an Apple
computer running Mac OS X 10.3.9 and MATLAB 7.2. For this machine, we
ran the fconfigure script with the following command:</p>

<p><code><pre>  ./configure --prefix=$HOME/ipopt/install          \
    ADD_CFLAGS="-fno-common -fexceptions -no-cpp-precomp -fPIC"   \
    ADD_CXXFLAGS="-fno-common -fexceptions -no-cpp-precomp -fPIC" \
    ADD_FFLAGS="-x f77-cpp-input -fPIC -fno-common"               \
    FLIBS="-lg2c -lfrtbegin -lSystem"                             \
    F77=g77 CC=gcc CXX=g++</pre></code></p>

<p>After this, follow the standard installation steps: type
<code>make</code>, wait a few minutes, then <code>make install</code>
in the UNIX command line. This compiles all the source code into a
single library and places it in the install directory as specified by
the <code>prefix</code> variable above.</p>

<p>What we haven't yet done is compile the code for the MATLAB
interface. We'll do this next.</p>

<p><b>Modify the Makefile and build the MEX file.</b> Go to into the
subdirectory <code>Ipopt/contrib/MatlabInterface/src</code> and open
the file called <code>Makefile</code> with your favourite text
editor. We need to change this file a little bit so that it coincides
with your MATLAB setup. You will find that most of the variables, such
as <code>CXX</code> and <code>CXXFLAGS</code>, have been automatically
(and hopefully, correctly) set according to the flags specified during
your initial call to <code>configure</code> script. However, you may
need to modify <code>MATLAB_HOME</code> and <code>MEXSUFFIX</code>, as
explained in the comments of the Makefile. On one of our Linux
machines, we had set these Makefile variables to</p>

<p><code><pre>  MATLAB_HOME = /cs/local/generic/lib/pkg/matlab-7.3/bin/matlab
  MEXSUFFIX   = mexglx
</pre></code></p>

<p>Once you think you've set up the Makefile properly, type <code>make
all</code> in the same directory as the Makefile. If you didn't get
any errors, then you're pretty much all set to go!</p>

<p>There's a great possibility you will encounter problems with the
installation instructions we have just described here. I'm afraid some
resourcefulness will be required on your part, as the installation
will be slightly different for each person. Please consult the <a
href="#trouble">troubleshooting section</a> on this webpage, and the
<a href="http://list.coin-or.org/pipermail/coin-ipopt">archives</a>
of the IPOPT mailing list. If you can't find the answer at either of
these locations, try sending an email to the <a
href="http://list.coin-or.org/mailman/listinfo/coin-ipopt">IPOPT
mailing list</a>.</p>

<p><b>Finally.</b> If the installation procedure was successful, you
will end up with a MEX file. On a Linux machine, the MEX file will be
called <code>ipopt.mexglx</code>. In order to use it in MATLAB, you
need to tell MATLAB where to find it. The best way to do this is to
type</p>

<p><code><pre>  addpath sourcedir</pre></code></p>

<p>in the MATLAB command prompt, where <code>sourcedir</code> is the
location of the MEX file you created. It is basically the full
pathname that ends in <code>Ipopt/contrib/MatlabInterface</code>. You
can also achieve the same thing by modifying the
<code>MATLABPATH</code> environment variable in the UNIX command line,
using either the <code>export</code> command (in Bash shell), or the
<code>setenv</code> command (in C-shell).</p>

<p><b>A note on 64-bit platforms.</b> Starting with version 7.3, 
MATLAB can handle 64-bit addressing, and the authors of MATLAB have 
modified the implementation of sparse matrices to reflect this change. 
However, the row and column indices in the sparse matrix are converted to
signed integers, and this could potentially cause problems when dealing 
with large, sparse matrices on 64-bit platforms with MATLAB version 
7.3 or greater.</p>

<a name="tutorial"><h3>Tutorial by example</h3>

<p>Let's go through four examples which demonstrate the principal
features of the MATLAB interface to IPOPT. For additional information,
you can always type <code>help ipopt</code> in the MATLAB prompt. The
tutorial exampels are all located in the directory
<code>Ipopt/contrib/MatlabInterface/examples</code>.</p>

<h4>Example 1</h4>

<p>First, let's look at the Hock & Schittkowski test problem #51.<a
href="#footnote1"><sup>1</sup></a> It is an optimization problem with
5 variables, no inequality constraints and 3 equality
constraints. There is a MATLAB script <code>examplehs051.m</code>
which runs the limited-memory BFGS (Broyden-Fletcher-Goldfarb-Shanno)
algorithm with the starting point <code>[2.5 0.5 2 -1 0.5]</code> and
obtains the solution <code>[1 1 1 1 1]</code>. The line in the script
which executes the IPOPT solver is</p>

<p><code><pre>  x = ipopt(x0,lb,ub,lbc,ubc,@computeObjective,@computeGradient,...
	    @computeConstraints,@computeJacobian,'',[],'',[],...
	    'jac_c_constant','yes','hessian_approximation',...
            'limited-memory','mu_strategy','adaptive','tol',1e-7);
</pre></code></p>

<p>The first input is the initial point. The second and third inputs
specify the lower and upper bounds on the variables. Since there are
no such bounds, we set the entries of these two vectors to
<code>-Inf</code> and <code>+Inf</code>. The fourth and fifth inputs
specify the lower and upper bounds on the 3 constraint functions. The
next five inputs specify the function handles to the required callback
routines. We've written the callback routines as subfunctions in the
same M-file. Since we are using a limited-memory approximation to the
Hessian, we don't need to know the values of the second-order partial
derivatives, so we set the Hessian callback routine to the empty
string. The rest of the input arguments set some options for the
solver, as detailed in the IPOPT documentation.</p>

<p>If you examine the functions <code>computeObjective</code> and
<code>computeGradient</code>, you will see that computing the
objective function and gradient vector is relatively
straightforward. The function <code>computeConstraints</code> returns
a vector of length equal to the number of constraint functions. The
callback function <code>computeJacobian</code> returns an M x N sparse
matrix, where M is the number of constraint functions and N is the
number of variables. It is important to always return a sparse matrix,
even if there is no computational advantage in doing so. Otherwise,
MATLAB will report an error.</p>

<h4>Example 2</h4>

<p>Let's move to the second example, <code>examplehs038.m</code>. It
demonstrates the use of IPOPT on an optimization problem with 4
variables and no constraints other than simple bound constraints. This
time, we've implemented a callback routine for evaluating the
Hessian. The Hessian callback function takes as input the current
value of the variables <code>x</code>, the factor in front of the
objective term <code>sigma</code>, an the values of the constraint
multipliers <code>lambda</code> (which in this case is empty). If the
last input is true, then the callback routine must return a sparse
matrix that has zeros in locations where the second-order derivative
at that location will ALWAYS be zero. The return value H must always
be a lower triangular matrix (type <code>help tril</code>). As
explained in the IPOPT documentation, the Hessian matrix is symmetric
so the information contained in the upper triangular part is
redundant.</p>

<p>This example also demonstrates the use an iterative callback
function, which can be useful for displaying the status of the
solver. This is specified by the twelfth input. The function
<code>callback</code> takes as input the current iteration
<code>t</code>, the current value of the objective <code>f</code>, and
the current point <code>x</code>.</p>

<h4>Example 3</h4>

<p>The third slightly more complicated example script is
<code>examplehs071.m</code>, which is the same as the problem explored
in the IPOPT documentation (Hock and Schittkowski test problem
#71). It is worth taking a peek at the functions
<code>computeHessian</code> and <code>computeJacobian</code>. In the
Hessian callback function, we make use of the input lambda. Since the
Hessian is dense, its structure is returned with the line</p>

<p><code><pre>   H = sparse(tril(ones(n)))</pre></code></p>

<p>where <code>n</code> is the number of variables. The Jacobian is
dense, so we can return its structure in a single line:</p>

<p><code><pre>   J = sparse(ones(m,n))</pre></code></p>

<p>where <code>m</code> is the number of constraint functions.</p>

<p>This example also differs from previous ones because the initial values for
the Lagrange multipliers are specified in MATLAB. We need to input three sets
of multipliers to IPOPT: the Lagrange multipliers corresponding to the lower
bounds on the optimization variables, the multipliers corresponding to the
uppwer bounds on the variables, and the multipliers associated with the
constraint functions. To specify these three sets of Lagrange multipliers, we
fill in three fields in the <code>multipliers</code> struct as follows:</p>

<p><code><pre>  multipliers.zl     = [1 1 1 1];
  multipliers.zu     = [1 1 1 1];
  multipliers.lambda = [1 1];</pre></code></p>

<p>Note that this optimization problem has 4 variables and 2 constraints.</p>

<p>In this example script, we have also chosen to output the the values of the
Lagrange multipliers that compose the dual portion of the final primal-dual 
solution. When I ran this script, I obtained final values of approximately</p>

<p><code><pre>  multipliers.zl     = [1 0 0 0];
  multipliers.zu     = [0 0 0 0];
  multipliers.lambda = [-0.55 0.16];</pre></code></p>

<p>The third output in the call to IPOPT is the number of iterations IPOPT
takes to converge to the stationary point within the specified tolerance.</p>

<h4>Example 4</h4>

<p>The last example is the script <code>examplelauritzen.m</code> in
the subdirectory <code>bayesnet</code>. It is vastly more complicated
than the other three. It pertains to research in inference in
probabilistic models in artificial intelligence. This script
demonstrates the problem of inferring the most probable states of
random variables given a Bayesian network.<a
href="#footnote2"><sup>2</sup></a> In this case, the model represents
the interaction between causes (e.g. smoking) and diseases (e.g. lung
cancer). We are given a patient that is a smoker, has tested positive
for some X-ray, and has recently visited Asia. In many ways this is a
silly and highly overused example, but it suits our needs here because
it demonstrates how to treat inference as an optimization problem, and
how to solve this optimization problem using IPOPT. This code should
NOT be used to solve large inference problems because it is not
particularly efficient.</p>

<p>The call to IPOPT is buried in the file <code>bopt.m</code>. It is</p>

<p><code><pre>[qR qS] = ipopt({qR qS},{repmat(eps,nqr,1) repmat(eps,nqs,1)},...
            { repmat(inf,nqr,1) repmat(inf,nqs,1) },...
            [ ones(1,nr) ones(1,ns) zeros(1,nc) ],...
            [ ones(1,nr) ones(1,ns) zeros(1,nc) ],...
            @computeJGObjective',@computeJGGradient',...
            @computeJGConstraints',@computeJGJacobian',...
            @computeJGHessian',{ K C f Rv Rf Sv Sf NS d },'',...
            ...</pre></code></p>

<p>As you can see, it is rather complicated! The first input, as
usual, is the starting point. (The variables actually represent
probability estimates.) Notice that we are passing a cell array, and
each entry of the cell array is a matrix. Likewise, the bound
constraints are specified as cell arrays. This is permitted as long as
the starting point and the bound constraints have the same
structure. If not, the MATLAB will report an error. (Note that the
lower bounds on the variables are set to floating-point
precision. Type <code>help eps</code>. In this way, we ensure that the
logarithm of a variable never evaluates to zero.)</p>

<p>This cell array syntax is useful when your program has several
different types of variables. These sets of variables are then passed
as separate input arguments to the MATLAB callback functions. For
instance, <code>qR</code> and <code>qS</code> are passed as separate
arguments to the objective callback function in the M-file
<code>computeJGObjective.m</code>.  The entries of the cell array are
also treated as separate outputs from the gradient callback function
(see the M-file <code>computeJGGradient.m</code>), and from the main
call to ipopt.</p>

<p>In this example, the constraint functions are all linear, so they have
no impact on the value of the Hessian. In fact, the Hessian is a
diagonal matrix (but not positive definite). The Jacobian can be
extremely large, but it is also very sparse; the number of entries is
a multiple of the number of variables.</p>

<p>This example also demonstrates the use of auxiliary data. In
<code>bopt.m</code>, notice that the input after
<code>computeJGHessian</code> is a cell array. This cell array is
passed as input to every MATLAB callback routine.</p>

<p>The tutorial is over!</p>

<a name="notes"><h3>Notes on implementation of the MATLAB interface</h3>

<p>We won't bore you with all the details of the implementation. We
would, however, like to briefly point out a few of them. Most of the
issues of interest surround the representation of sparse matrices.</p>

<p>The MATLAB interface will necessarily be slower than the standard
C++ interface to IPOPT. That's because MATLAB dynamically allocates
new memory for all the outputs passed back from a function. Thus, for
large problems each iteration of IPOPT will involve the dynamic
allocation and deallocation of large amounts of memory.</p>

<p><b>Sparse matrices.</b> The greatest challenge was most definitely
the conversion of sparse matrices from MATLAB to IPOPT. Sparse
matrices are used to represent the Jacobian of the constraint
functions and the Hessian of the Lagrangian function. There is a very
nice <a href="http://link.aip.org/link/?SML/13/333">document</a> by
Gilbert, Moler and Schreiber that discusses the design and
implementation of sparse matrices in the MATLAB environment. The
problem is that IPOPT assumes a static sparse matrix structure, but in
MATLAB there is no way to ensure that the size of the matrix (the
number of non-zero elements) does not change over time; if an entry of
a sparse matrix is set to zero, then the arrays are automatically
adjusted so that no storage is expended for that entry. This may seem
like a highly inefficient way to implement sparse matrices, and indeed
it is. However, Gilbert, Moler and Schreiber emphasize efficient
matrix-level operations over efficient element-level operations.</p>

<p>We can legitimately make the following assumption: the non-zero
entries of a sparse matrix passed back from MATLAB are a
<emph>subset</emph> of the non-zero entries in IPOPT's respective
sparse matrix.</p>

<p>The class <code>SparseMatrixStructure</code> keeps track of the
structure of a sparse MATLAB matrix. It does not store the values of
the non-zero entries. We use it for the Jacobian of the constraints
and the Hessian of the Lagrangian. Even though these two matrices are
fundamentally different (one is square and lower triangular, the other
is rectangular), we can treat their structures in the same way.</p>

<p>The principal functions of interest in class
<code>SparseMatrixStructure</code> are <code>getColsAndRows</code> and
<code>copyElems</code>. The function <code>getColsAndRows</code>
converts the MATLAB sparse matrix format into the equivalent IPOPT
format. The function <code>copyElems</code> copies the entries from
one sparse matrix to another when one sparse matrix has a different
structure than the other. Obviously, for the copy operation to be
plausible, the set of non-zero entries of the destination matrix must
be a superset of the non-zero entries of the source matrix. Due to
efficiency concerns, no checks are made to ensure that this is
satisfied; it is up to the user to ensure that a sparse matrix passed
back to IPOPT never has a non-zero entry that was not declared
initially when <code>returnStructureOnly = true</code>. In my
implementation, I have gone through great pains to ensure: 1. that the
copy function only makes one pass through the entries of the sparse
matrix, and 2. that there are no if statements inside the for loops,
which can severely impinge on the speed of the copy operation.</p>

<a name="trouble"><h3>Troubleshooting and known issues</h3>

<p>The installation procedure we have described does involve a certain
amount of expertise on the part of the user. If you are encountering
problems, it is highly recommended that you follow the standard
installation of IPOPT first, and then test the installation by running
some examples, either in C++ or in AMPL.</p>

<p>What follows are a list of common errors encountered, along
with a suggested remedy:</p>

<ul>

<li><b>Compilation is successful, but MATLAB crashes.</b> Even if you
didn't get any errors during compilation, there's still a possibility
that you didn't link the MEX file properly. In this case, executing
IPOPT in MATLAB will cause MATLAB to crash. This is a common problem,
and usually arises because you did not choose the proper compiler or
set the proper compilation flags (e.g. <code>-fPIC</code>) when you
ran the <code>configure</code> script at the very beginning.

<li><b>MATLAB fails to link to IPOPT shared library.</b> You might 
encounter this problem if you try to execute one of the examples in 
MATLAB, and MATLAB complains that it cannot find the IPOPT shared 
library. The installation script has been set up so that the MEX file 
you are calling knows where to look for the IPOPT shared library.
However, if you moved the library then you will run into a problem.
One way to fix this problem is to modify the <code>LDFLAGS</code> 
variable in the MATLAB interface Makefile (see above) so that the 
correct path of the IPOPT library is specified. Alternatively, you 
could modify the <code>LD_LIBRARY_PATH</code>
environment variable so that the location of the IPOPT library is
included in the path. If none of this is familiar to you, you might 
want to do a web search to find out more.

<li><b>&quot;mwIndex&quot; is not defined.</b> You may get a
compilation error that says something to the effect that mwIndex is
not defined. This error will surface on a version of MATLAB prior to
7.3. The solution is to add the flag <code>-DMWINDEXISINT</code> to 
the <code>CXXFLAGS</code> variable in the MATLAB interface Makefile 
(see above).

</ul>

<h3>Footnotes</h3>

<p><a name="footnote1"><sup>1</sup></a> For
further information, see: Willi Hock and Klaus Schittkowski. (1981)
Test Examples for Nonlinear Programming Codes. Lecture Notes in
Economics and Mathematical Systems Vol. 187, Springer-Verlag.</p>

<p><a name="footnote2"><sup>2</sup></a> If you want to learn more
     about junction graphs for approximate inference, and a special
     case of this is called the <emph>junction tree
     algorithm</emph>. Recommended reading includes: <ul><li>Aji and
     McEliece (2001). The generalized distributive law and free energy
     minimization. <i>Proceedings of the 39th Allerton Conference.</i>
     <li> Yedidia, Freeman and Weiss (2005). Constructing free-energy
     approximations and generalized belief propagation algorithms.
     <i>IEEE Transactions on Information Theory</i> 51(7).</ul></p>

</html>
